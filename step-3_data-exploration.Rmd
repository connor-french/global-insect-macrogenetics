---
title: "Data exploration"
output: 
  html_document:
    toc: true
---

## Setup
Load packages
```{r setup, message=FALSE, warning=FALSE}
library(raster)
library(tidyverse)
library(tidymodels)
library(sf)
library(here)
library(wesanderson)
library(rnaturalearth)
library(patchwork)
library(blockCV)
library(tictoc)
library(vip)
library(furrr)
library(spdep)
library(ncf)
library(corrr)
library(rstan)
library(tidybayes)
library(bayesplot)
library(BBmisc)
library(rstanarm)
library(glmmfields)
library(projpred)
library(data.table)
library(corrr)

source("R/helper_functions.R")
```

### Map helpers
```{r map-helpers}
pal <- wes_palette("Zissou1", 100, type = "continuous")

# for assigning cells to continents. Islands are missed at coarser resolutions
world_base <- rnaturalearth::ne_countries(scale = "large", returnclass = "sf") %>%
  select(continent, name_long) %>% 
  st_transform(crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs") 

# for mapping. this will be smaller so plotting is faster
world_base_map <- rnaturalearth::ne_countries(scale = "small", returnclass = "sf") %>%
  select(continent, name_long) %>% 
  st_transform(crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs") 
```


## Data wrangling
Read in the data. Two main data sources- the genetic summary statistics and the environmental data. 
```{r data}
sumstats <- read_csv(here("output", "spreadsheets", "cell_medium_3_10_sumstats.csv"))

rast_list_medium <- list.files(here("data", "climate_agg"),
                        pattern = "medium",
                        full.names = TRUE)

rasters_full_medium <- raster::stack(rast_list_medium)
crs(rasters_full_medium) <- "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs"

glimpse(sumstats)
rasters_full_medium
```

Write raster maps to file for visual inspection.
```{r}
pdf(file = here("output", "exploratory_plots", "predictor_rasters.pdf"))
for (i in 1:nlayers(rasters_full_medium)) {
  plot(rasters_full_medium[[i]], main = names(rasters_full_medium[[i]]))
  plot(st_geometry(world_base_map), add=TRUE)
}
dev.off()

```




Extract raster values for each cell that has genetic summary data and join the data frames for a full data set. 
```{r full-data}
explanatory_df <- rasters_full_medium[sumstats$cell] %>% 
  as_tibble() %>% 
  mutate(cell = sumstats$cell) 
  

full_df <- left_join(sumstats, explanatory_df, by = "cell") %>% 
  # convert soil classifications to a factor
  mutate(soil_medium_hwsd = as.factor(soil_medium_hwsd)) %>% 
  # arrange for plotting later
  arrange(cell)

glimpse(full_df)
```

Convert this to a `sf` polygon object for mapping and spatial cross validation. Also, extracting the continent for each polygon (largest overlap) to assess sampling imbalance.
```{r continents}
template_medium <- raster(here("data", "templates", "template_medium.tif"))

template_medium[full_df$cell] <- full_df$avg_pi

full_sf <- rasterToPolygons(template_medium) %>% 
  st_as_sf() %>% 
  bind_cols(full_df) %>% 
  select(-template_medium) %>%
  # Adding continent column to the data frame.
  st_join(world_base["continent"], largest = TRUE)


glimpse(full_sf)
```

Visualizing temperature to make sure the conversion was successful. 
```{r temp-vis}
ggplot() +
  geom_sf(data = full_sf, aes(fill = current_medium_bio_1, color = current_medium_bio_1)) +
  scale_fill_gradientn(colors = pal) +
  scale_color_gradientn(colors = pal) + 
  theme_minimal()
```


### Continent mapping
How many cells successfully mapped to a continent, and what is the sample size? Looks like 2 cells did not map correctly.
```{r continent-count}
full_sf %>% 
  count(continent)
```

Visualize which cells mapped correctly. All of the "Open Ocean" values are islands around Africa. Although these may politically be assigned a different continent (e.g. some islands around Madagascar are European), spatially they're near Africa, so I'm classifying them as African.  
```{r continent-map, cache=TRUE}
ggplot() + 
  geom_sf(data = world_base_map) +
  geom_sf(data = full_sf, aes(fill = continent, color = continent))
```

Converting the cells and taking another look. Looks fine, except for the NA. 
```{r continent-map-2, cache=TRUE}
full_sf <- full_sf %>% 
  mutate(continent = ifelse(str_detect(continent, "Seven"), "Africa", continent))

ggplot() + 
  geom_sf(data = world_base_map) +
  geom_sf(data = full_sf, aes(fill = continent, color = continent))
```


Let's see where the NA is. Looks like South America! 
```{r na-find, cache=TRUE}
full_sf %>% 
  mutate(na_cont = if_else(is.na(continent), "Missing", "Present")) %>% 
  ggplot() + 
  geom_sf(aes(fill = na_cont, color = na_cont))
```

Replacing the NA value with "South America". North America has the most representation, while South America has the least.
```{r continent-sampling}
full_complete <- full_sf %>% 
  mutate(continent = if_else(is.na(continent), "South America", continent))

count(full_complete, continent) %>% mutate(perc = n / sum(n)) %>% 
  as_tibble() %>% 
  select(continent, n, perc) %>%
  arrange(desc(perc)) %>% 
  knitr::kable()
```

### Predictor variables

Plant phylogenetic diversity has an excessive number of NAs (56). May need to throw out. Others have between 0 and 11 NAs. Need to see if these are for the same cells or not
```{r missing-data}
full_complete %>% 
  summarize_all(~sum(is.na(.))) %>% 
  pivot_longer(cols = !contains("geometry"), 
               names_to = "variable",
               values_to = "num_NA") %>% 
  filter(num_NA > 0)
```

It seems like there is some overlap with the missing data, but we're going to have to throw out 9 cells.
```{r}
nrow_filt <- full_complete %>% 
  select(-plant_medium_plant_phylo) %>% 
  na.omit() %>% 
  nrow()

nrow(full_complete) - nrow_filt
```

#### Plant richness
Let's visualize where the plant richness diversity missing data is. It looks like the missing data is restricted to somewhat randomly distributed islands, including New Zealand. Let's explore a bit more to see if it would be worth diving into the data to fill in the missing values.
```{r}
full_complete %>% 
  mutate(plant_missing = 
           if_else(is.na(plant_medium_plant_phylo), "Missing", "Present")) %>% 
  ggplot() + 
  geom_sf(aes(fill = plant_missing, color = plant_missing))
```

If there's no linear relationship with a Hill One or Average pi, then I'm just going to throw it out.

Plots don't look like much:
```{r}
p_hill <- ggplot(data = full_complete, aes(x = plant_medium_plant_phylo, y = hill_1)) + 
  geom_point() + 
  geom_smooth()

p_pi <- ggplot(data = full_complete, aes(x = plant_medium_plant_phylo, y = avg_pi)) + 
  geom_point() + 
  geom_smooth()


p_hill / p_pi
```

Linear models also show no real relationship (avg_pi is significant, but R2 is 0.02)
```{r}
summary(lm(hill_1 ~ plant_medium_plant_phylo, data = full_complete))

summary(lm(avg_pi ~ plant_medium_plant_phylo, data = full_complete))
```

**Decision**: Removing the plant richness as a predictor.  

#### Missing data
Let's see what the distribution of cells with an NA is, sans-plant richness. Most are islands, and there are a few from the Arctic.
```{r, cache=TRUE}
full_missing <- full_complete %>% select(-plant_medium_plant_phylo)

full_missing <- full_missing[rowSums(is.na(full_missing)) > 0,]

ggplot() +
  geom_sf(data = world_base_map) +
  geom_sf(data = full_missing, aes(fill = continent, color = continent))
```

What is the sample size per cell? These are info-rich cells, but there are too many variables to get this info back from to where I don't think the effort is worth it. I'm going to remove them and call it good.
```{r}
full_missing %>% select(continent, num_otu, num_ind, num_order)
```


Remove the plant richness column and filter NAs. Also adding in latitude and longitude columns.
```{r}
full_filter <- full_complete %>% 
  select(-plant_medium_plant_phylo) %>% 
  remove_missing() 

# get centroid coordinate for each cell
coords <- st_centroid(full_filter) %>% 
  st_coordinates()

full_filter <- full_filter %>% 
  mutate(longitude = coords[,1],
         latitude = coords[,2])

glimpse(full_filter)
```

Map the latitude to make sure I got the correct column.
```{r}
ggplot() +
  geom_sf(data = full_filter, aes(fill = latitude, color = latitude)) +
  scale_fill_gradientn(colors = pal) + 
  scale_color_gradientn(colors = pal)
```

### Data exploration

#### Read and filter
Read in genetic summary data for the least restrictive high resollution and medium resolution genetic filtering regimes. I can impose more restrictive filtering protocols from there.
```{r}
# least restrictive 3 individual filtering regimes
#high_3_df <- read_csv(here("output", "spreadsheets", "cell_high_3_10_sumstats.csv"))
med_3_df <- read_csv(here("output", "spreadsheets", "cell_medium_3_10_sumstats.csv"))
#low_3_df <- read_csv(here("output", "spreadsheets", "cell_low_3_10_sumstats.csv"))

# least restrictive 5 individual filtering regime
#high_5_df <- read_csv(here("output", "spreadsheets", "cell_high_5_10_sumstats.csv"))



```

Read in predictor variables
```{r}
# rast_list_high <- list.files(here("data", "climate_agg"),
#                         pattern = "high",
#                         full.names = TRUE)
# 
# rasters_full_high <- raster::stack(rast_list_high)
# crs(rasters_full_high) <- "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs"


rast_list_medium <- list.files(here("data", "climate_agg"),
                        pattern = "medium",
                        full.names = TRUE)

rasters_full_medium <- raster::stack(rast_list_medium)
crs(rasters_full_medium) <- "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs"

# rast_list_low <- list.files(here("data", "climate_agg"),
#                         pattern = "_low",
#                         full.names = TRUE)
# 
# rasters_full_low <- raster::stack(rast_list_low)
# crs(rasters_full_low) <- "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs"

```

Join the predictors with the genetic summaries.
```{r}
join_predictors <- function(gen_sumstats, pred_rasts, resolution){
  exp_df <- pred_rasts[gen_sumstats$cell] %>% 
    as_tibble() %>% 
    mutate(cell = gen_sumstats$cell)
  
  if (resolution == "high") {
    full_df <- left_join(gen_sumstats, exp_df, by = "cell") %>% 
      mutate(soil_high_hwsd = as.factor(soil_high_hwsd),
             resolution = "high") %>% 
      arrange(cell)
  } else if (resolution == "medium") {
    full_df <- left_join(gen_sumstats, exp_df, by = "cell") %>% 
      mutate(soil_medium_hwsd = as.factor(soil_medium_hwsd),
             resolution = "medium") %>% 
      arrange(cell) 
  } else {
    full_df <- left_join(gen_sumstats, exp_df, by = "cell") %>% 
      mutate(resolution = "low") %>% 
      arrange(cell) 
  }
  
  
  return(full_df)
}

# high_3_full <- join_predictors(high_3_df, rasters_full_high, "high")
# high_5_full <- join_predictors(high_5_df, rasters_full_high, "high")
med_3_full <- join_predictors(med_3_df, rasters_full_medium, "medium")
# low_3_full <- join_predictors(low_3_df, rasters_full_low, "low")


# nrow(high_3_full)
# nrow(high_5_full)
nrow(med_3_full)
# nrow(low_3_full)

```

Filter each dataset, removing the plant species richness and removing NAs.
```{r}
filter_dfs <- function(df, resolution) {
  plant_res <- paste0("plant_", resolution, "_plant_phylo")
  df_filtered <- df %>% 
    select(-c(plant_res)) %>% 
    remove_missing()
  return(df_filtered)
}

# high_3_filter <- filter_dfs(high_3_full, "high")
# high_5_filter <- filter_dfs(high_5_full, "high")
med_3_filter <- filter_dfs(med_3_full, "medium")
# low_3_filter <- filter_dfs(low_3_full, "low")


# nrow(high_3_filter)
# nrow(high_5_filter)
nrow(med_3_filter)
# nrow(low_3_filter)
```


#### PCA
Given that classes of variables are often highly correlated, I'm going to perform principle component analysis on sets of predictor variables and create new composite variables to investigate.  
Before performing the PCA on each data set, I'm going to create different data subsets, imposing stricter limits on the minimum number of OTUs per filtering regime.
I'm doing this for: current temperature, current precipitation, global habitat heterogeneity, terrain continuous average, terrain continuous standard deviation, and terrain categorical (percentages).
```{r}
all_dfs <- tibble(
  resolution = c(#paste(rep("high", 12, sep = ", ")), 
                 paste(rep("medium", 6, sep = ", "))#,
                 #paste(rep("low", 6, sep = ", "))
                 ),
  min_ind = c(3L, 3L, 3L, 3L, 3L, 3L, 
              #5L, 5L, 5L, 5L, 5L, 5L, 

              # 3L, 3L, 3L, 3L, 3L, 3L
              ),
  min_otu = c(10L, 25L, 50L, 100L, 150L, 200L),
  df = list(
    # high_3_filter,
    # high_3_filter %>% filter(num_otu >= 20),
    # high_3_filter %>% filter(num_otu >= 50),
    # high_3_filter %>% filter(num_otu >= 100),
    # high_3_filter %>% filter(num_otu >= 150),
    # high_3_filter %>% filter(num_otu >= 200),
    # high_5_filter,
    # high_5_filter %>% filter(num_otu >= 20),
    # high_5_filter %>% filter(num_otu >= 50),
    # high_5_filter %>% filter(num_otu >= 100),
    # high_5_filter %>% filter(num_otu >= 150),
    # high_5_filter %>% filter(num_otu >= 200),
    med_3_filter,
    med_3_filter %>% filter(num_otu >= 25),
    med_3_filter %>% filter(num_otu >= 50),
    med_3_filter %>% filter(num_otu >= 100),
    med_3_filter %>% filter(num_otu >= 150),
    med_3_filter %>% filter(num_otu >= 200)
    # low_3_filter,
    # low_3_filter %>% filter(num_otu >= 20),
    # low_3_filter %>% filter(num_otu >= 50),
    # low_3_filter %>% filter(num_otu >= 100),
    # low_3_filter %>% filter(num_otu >= 150),
    # low_3_filter %>% filter(num_otu >= 200)
  )
) %>% 
  mutate(num_cells = map_int(df, nrow))

### function to conduct a pca for each variable set and each data frame and add the new PCA variables back to the original data frames

get_pc_scores <- function(df) {
  ### vectors of variables to perform pca on
  all_vars <- colnames(df)
  
  if (str_detect(all_vars[15], "high")) {
    # temperature bioclims
    temp_vars <- paste0("current_high_bio_", 1:11)
    # precipitation bioclims
    precip_vars <- paste0("current_high_bio_", 12:19)
  } else if (str_detect(all_vars[15], "medium")) {
    temp_vars <- paste0("current_medium_bio_", 1:11)
    # precipitation bioclims
    precip_vars <- paste0("current_medium_bio_", 12:19)
  } else if (str_detect(all_vars[15], "_low_")) {
    temp_vars <- paste0("current_low_bio_", 1:11)
    # precipitation bioclims
    precip_vars <- paste0("current_low_bio_", 12:19)
  }
  
  # global habitat heterogeneity
  ghh_vars <- all_vars[str_starts(all_vars, "ghh_")]
  
  # terrain continuous average
  terr_median_vars <- all_vars[str_ends(all_vars, "_median")]
  
  # terrain continuous standard deviation
  terr_sd_vars <- all_vars[str_ends(all_vars, "_sd")]
  
  # terrain categorical
  terr_cat_vars <- all_vars[str_detect(all_vars, "_geom")]
  
  # combine all into a single named list for looping
  predictor_list <- list(
    temp = temp_vars,
    precip = precip_vars,
    ghh = ghh_vars,
    terr_median = terr_median_vars,
    terr_sd = terr_sd_vars,
    terr_cat = terr_cat_vars
  )
  
  # function to perform the actual PCA
  perf_pca <- function(preds) {
    df_preds <- df %>% 
      select(all_of(preds))
    
    pca <- prcomp(df_preds, center = TRUE, scale. = TRUE)$x %>%
      as_tibble() %>%
      # retain the first two PCs
      select(paste0("PC", 1:2))
    
    return(pca)
  }
  
  # perform the PCA across variable sets
  pca_list <- invisible(map(predictor_list, perf_pca))
  
  names(pca_list) <- names(predictor_list)
  
  # assign prefixes to the PC column names so each variable set has distinct column names
  for (name in names(pca_list)) {
    colnames(pca_list[[name]]) <- paste0(name, "_", colnames(pca_list[[name]]))
  }

  out_df <- bind_cols(df, pca_list)

  return(out_df)
}

all_dfs_pca <- all_dfs %>% 
  mutate(df = map(df, get_pc_scores))

```


Normalize all variables so they are centered (mean of 0) and scaled (sd = 1) for linear regression.
```{r}
normalize_vars <- function(df_in) {
  norm_rec <- recipe(hill_1 ~ ., data = df_in) %>%
    step_normalize(all_numeric(), -cell, -num_otu, -num_ind, -num_order, -contains("_pi"), -contains("hill_"))
  
  df_trans <- norm_rec %>%
    prep(training = df_in) %>%
    juice(all_predictors()) %>%
    mutate(hill_1 = df_in$hill_1)
}

all_dfs_norm <- all_dfs_pca %>% 
  mutate(df_norm = map(df, normalize_vars),
         filter_regime = paste(resolution, min_ind, min_otu, sep = "_")) %>% 
  select(-df)


```

#### Map
I'm filtering out data sets that I already know I'm not going to use.
```{r}
all_dfs_norm <- all_dfs_norm %>% 
  filter(resolution == "medium",
         min_otu >= 100)
```


Convert to sf
```{r}
# functions to convert the data frames to sf objects
to_sf <- function(df) {
  if (df$resolution[1] == "high") {
    template <- raster(here("data", "templates", "template_high.tif"))
  } else if (df$resolution[1] == "medium") {
    template <- raster(here("data", "templates", "template_medium.tif"))
  } else {
    template <- raster(here("data", "templates", "template_low.tif"))
  }
  
  template[df$cell] <- df$num_order
  
  df_sf <- rasterToPolygons(template) %>% 
    st_as_sf() %>% 
    bind_cols(df) %>% 
    st_join(world_base["continent"], largest = TRUE)
  
  return(df_sf)
}


# convert the data frames to sf for plotting
all_dfs_sf <- all_dfs_norm %>% 
  mutate(df_sf = map(df_norm, to_sf))
```


Plot of spatial distribution for each filtering regime. Since there are so many maps, I am plotting two exemplars: Hill 1 for medium resolution with minimum 100 OTUs per cell and minimum 150 OTUs per cell. There's definitely spatial autocorrelation with Hill 1.
```{r}
#dfs_high <- filter(all_dfs_sf, resolution == "high")
dfs_medium <- filter(all_dfs_sf, resolution == "medium")
#dfs_low <- filter(all_dfs_sf, resolution == "low")
# 
# plot_maps_high <- function(df) {
#   ggplot() +
#     geom_sf(data = world_base_map, alpha = 0.5) +
#     geom_sf(
#       data = all_dfs_sf$df_sf[[1]],
#       fill = "grey",
#       color = "grey",
#       alpha = 0.7
#     ) +
#     geom_sf(data = df, aes(fill = hill_1, color = hill_1)) +
#     scale_fill_viridis_c() +
#     scale_color_viridis_c() +
#     theme_minimal()
# }

plot_maps_medium <- function(df) {
  ggplot() +
    geom_sf(data = world_base_map, alpha = 0.5) +
    geom_sf(
      data = all_dfs_sf$df_sf[[1]],
      fill = "transparent",
      color = "grey",
      alpha = 0.7
    ) +
    geom_sf(data = df, aes(fill = hill_1, color = hill_1)) +
    scale_fill_viridis_c() +
    scale_color_viridis_c() +
    labs(title = paste0("Number of cells: ", nrow(df))) +
    ggthemes::theme_map()
}

# plot_maps_low <- function(df) {
#   ggplot() +
#     geom_sf(data = world_base_map, alpha = 0.5) +
#     geom_sf(
#       data = all_dfs_sf$df_sf[[19]],
#       fill = "transparent",
#       color = "grey",
#       alpha = 0.7
#     ) +
#     geom_sf(data = df, aes(fill = hill_1, color = hill_1)) +
#     scale_fill_viridis_c() +
#     scale_color_viridis_c() +
#     labs(title = paste0("Number of cells: ", nrow(df))) +
#     ggthemes::theme_map()
# }


#maps_high <- map(dfs_high$df_sf, plot_maps_high)
maps_medium <- map(dfs_medium$df_sf, plot_maps_medium)
#maps_low <- map(dfs_low$df_sf, plot_maps_low)

maps_medium[[1]]

maps_medium[[2]]
```

#### Tropical-temperate divide
I'm adding a variable that indicates whether the cell is above or below the frost line (min temp of coldest month <= 0 degrees C). [White et al. 2019](https://doi.org/10.1038/s41467-019-10253-6) found that there is strong turnover in community composition across this line.  

First, read in and transform the data
```{r}
div_sf <- st_read(here("data", "climate_poly", "min_temp_binary.geojson")) %>% 
  st_transform(crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs")

plot(div_sf)
```

Extract data for temperate-tropical divide.
```{r}
# hack to validate the multipolygon
div_sf_buf <- st_buffer(div_sf, dist = 0)

dfs_medium <- dfs_medium %>% 
  mutate(df_sf = map(df_sf, st_join, y = div_sf_buf, largest = TRUE))

```

```{r}
ggplot() +
  geom_sf(data=dfs_medium$df_sf[[1]], aes(fill = min_temp, color= min_temp))
```

```{r}
ggplot() +
  geom_boxplot(data=dfs_medium$df_sf[[1]], aes(y = hill_1, x = min_temp))
```

#### Species richness
SPECIES RICHNESS LOOKS TO JUST BE SAMPLING EFFORT

I'm adding insect species richness as a predictor for genetic diversity.

```{r}
insect_sp_richness <- st_read(here("data", "climate_poly", "insect_sp_richness.geojson"))

st_crs(insect_sp_richness) <- st_crs(dfs_medium$df_sf[[1]])

insect_sp_richness <- insect_sp_richness %>% 
  mutate(log_sp = log(n_species))
```

Looks like there sampling bias in the species richness estimate.
```{r}
ggplot() +
  geom_sf(data = world_base_map) +
  geom_sf(data = insect_sp_richness, aes(fill = log_sp, color = log_sp)) +
  scale_fill_viridis_c() +
  scale_color_viridis_c()
```

Add species richness to original data frame.
```{r}
dfs_medium <- dfs_medium %>% 
  mutate(df_sf = map(df_sf, st_join, y = insect_sp_richness, join = st_equals))
```


```{r}
ggplot() +
  geom_sf(data = dfs_medium$df_sf[[1]], aes(fill = log_sp, color = log_sp))
```



#### Spatial autocorrelation

Investigating spatial autocorrelation of response variables with Moran's I. First going to check out spatial autocorrelation of filtering regime we're likely to use (medium resolution, 150 OTU minimum). 

Plotting the neighborhood list to view the neighbors. The neighbors appear to be pretty tight. Going to want to view a correlogram at some point to see where spatial autocorrelation decays.
```{r}
sp_auto_df <- all_dfs_sf %>% 
  filter(filter_regime == "medium_3_150") %>% 
  pull(df_sf) %>% 
  pluck(1) %>% 
  select(hill_1) 

# create a neighborhood matrix (queen = TRUE means all neighbors, including diagonals, will be included)
sp_nb <- poly2nb(sp_auto_df, queen = TRUE)

# create a weights matrix. style = "W" means that the weights will be scaled from 0-1. This way we can compare across areas with different numbers of areas, which is true for this data set.
# also, ignoring cases with no neighbors. Errors would get thrown otherwise, and since we have islands with no neighbors, this would be a problem
sp_w <- nb2listw(sp_nb, style = "W", zero.policy = TRUE)



plot(sp_nb, coords = st_centroid(sp_auto_df) %>% st_coordinates())
```


Based on these Moran's I plots, it looks like there is spatial autocorrelation in Hill 1 under this strict neighborhood scheme. I plotted it two ways- including zeros and excluding them (turning them to NAs which is why the error appears). Excluding them shows clear spatial patterning;
```{r}
moran.plot(sp_auto_df$hill_1, sp_w,
           xlab = "Hill 1",
           ylab = "Neighbors Hill 1",
           zero.policy = TRUE)

moran.plot(sp_auto_df$hill_1, sp_w,
           xlab = "Hill 1",
           ylab = "Neighbors Hill 1",
           zero.policy = FALSE)


```

First, a quick Moran test to check for spatial autocorrelation. It's better to use the MCMC method since the regression-based method has some strict assumptions that our data likely violates. 

Hill 1 definitely has spatial autocorrelation under this neighborhood scheme.
```{r}
moran.mc(sp_auto_df$hill_1, sp_w, nsim = 1000, zero.policy = TRUE)
```


Now let's plot a spatial correlogram.
```{r}
sp_corr <- sp.correlogram(sp_nb, sp_auto_df$hill_1, order = 7, method = "I", randomisation = TRUE, zero.policy = TRUE)

print(sp_corr)
plot(sp_corr)
```


Moran's I touches zero at around 1,500 km.
```{r}
sp_coords <- st_centroid(sp_auto_df) %>% st_coordinates()
sp_moran_corr <- ncf::spline.correlog(x = sp_coords[,1], 
                                      y = sp_coords[,2], 
                                      z = sp_auto_df$hill_1,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)

plot(sp_moran_corr)
```

I'm going to conduct moran's I tests from 500 km to 1500 km to see where spatial autocorrelation becomes insignificant.

I'm plotting the p-values vs distance. It looks like spatial autocorrelation reliably drops off at around 1,500 km.
```{r}
set.seed(998)
sp_correlog_test <- correlog(x = sp_coords[,1], 
                             y = sp_coords[,2], 
                             z = sp_auto_df$hill_1, 
                             increment = 10000, 
                             resamp = 1000, 
                             latlon = FALSE,
                             na.rm = TRUE, 
                             quiet = FALSE)

sp_correlog_df <- tibble(
  distance = sp_correlog_test$mean.of.class,
  pval = sp_correlog_test$p
  ) %>% 
  filter(distance >= 5e5, distance < 1.7e6)



ggplot(data = sp_correlog_df, aes(x = distance, y = pval)) +
  geom_point() +
  geom_hline(yintercept = 0.05, color = "red") + 
  theme_minimal()

```



#### Correlations
Exploring correlations among variables. Need to decide which to keep and which to throw out. I'm exploring the medium resolution, 150 km data set.  

Here are the variables I value:
**Climate**- Including all current bioclims in the correlation matrix. I am prioritizing the extremes (e.g. max temp of warmest month) and average (e.g. average annual temp). Katie says seasonality shouldn't have a huge effect since insects tend to aestivate/hibernate when conditions aren't ideal. However, since insects are ectotherms, extremes likely represent limits to insect tolerances and averages summarize the overall climate regime of the area.

**Habitat**- I have two datasets summarizing habitat variability: the dynamic habitat indices and habitat heterogeneity. For both, I am prioritizing measures of spatial heterogeneity, followed by average, followed by seasonality. The habitat heterogeneity measures only correspond with spatial heterogeneity.  

**Terrain**- I am limiting terrain to slope median and standard deviation and elevation median and standard deviation. The rest are derived stats that I couldn't justify using. Prioritizing sd since variation likely drives genetic diversity more than average

**Land Cover**- No land cover: highly spatially autocorrelated variables that only make sense to use in conjunction with each other.  

**Human**- only doing human modification since it's a specific measure of human environmental impact, rather than just human density  

**Stability**- including both temperature and precipitation stability. I doubt they're correlated. Keeping temperature if so, since precipitation is more difficult to model in past climates


```{r}
df_corr <- all_dfs_norm %>% 
  filter(filter_regime == "medium_3_150") %>% 
  pull(df_norm) %>% 
  pluck(1) %>% 
  select(hill_1,
         num_ind,
         num_otu,
         num_order,
         contains("_pi"),
    -contains("land_cover"),
         contains("elevation"),
         contains("slope"),
         contains("current"),
         contains("ghh"),
         contains("dhi"),
    contains("gHM"),
    contains("stability")) %>% 
  corrr::correlate()

```

I'm visualizing each set of variables separately first to make the correlations easier to interpret.

##### Climate

Keeping: 
BIO2 (mean diurnal range). It's uncorrelated with all other variables
BIO5, BIO6 (max temp warmest month, min temp coldest month). They're uncorrelated with each other and aren't strongly correlated with many other variables. BIO5 is strongly correlated with BIO1, so I'm throwing out BIO1 since extremes are higher priority.
BIO7 (temperature annual range). Represents extremes across the year.
BIO13, BIO14 (precipitation of wettest month, precipitation of driest month).
BIO15 (precipitation seasonality). Uncorrelated with BIO13 and BIO14. Also, precipitation seasonality varies regardless of temperate vs tropical regions, so maybe relevant on a global scale.

```{r}
# climate correlation matrix
remove_prefix <- function(x, pref = "current_medium_") {
  s <- str_remove_all(x, pref)
  return(s)
}

corr_clim <- df_corr %>% 
  filter(str_detect(term, "current")) %>% 
  select(term, contains("current")) %>% 
  rename_at(vars(contains("current")), remove_prefix) %>% 
  mutate(rowname = str_remove_all(term, "current_medium_"))

corrr::rplot(corr_clim)
```

##### Habitat
I'm selecting cumulative DHI, var DHI, variance GHH, and standard deviation GHH, as these are uncorrelated with each other. While minimum DHI could be considered an "extreme", it does not reflect physiological limits, so since it was correlated with both cum DHI and var DHI, while those two variables are not correlated with each other, I'm going to select them since they likely explain more independent information than min DHI. There were many options for GHH, but variance was correlated with the fewest variables and standard deviation is uncorrelated with variance and also is a recognizable measure of variation.

```{r}

corr_hab <- df_corr %>% 
  filter(str_detect(rowname, "ghh|dhi"),
         !str_detect(rowname, "PC")) %>% 
  select(rowname, contains("ghh"), contains("dhi"), -contains("PC")) %>% 
  rename_at(vars(contains("ghh")), ~remove_prefix(.x, "_medium")) %>%
  rename_at(vars(contains("dhi")), ~remove_prefix(.x, "_medium")) %>%
  mutate(rowname = str_remove_all(rowname, "_medium"))


corrr::rplot(corr_hab)
```


##### Terrain
Retaining elevation median and standard deviation. Both correlate with slope median and sd. Elevation likely matters more than slope.
```{r}
corr_terr <- df_corr %>% 
  filter(str_detect(rowname, "elevation|slope"),
         !str_detect(rowname, "geom")) %>% 
  select(rowname, contains("elevation"), contains("slope"), -contains("geom")) 

corrr::rplot(corr_terr)
```

##### Refined correlation matrix
Now I'm going to look at a correlation matrix of this reduced set of variables. Priority is climate > habitat > human > terrain for selection. Climate variables most likely translate to a larger scale the best, and terrain variables are likely the most indirect predictors of genetic diversity. Habitat is probably most directly relevant, but the noise in the variables is likely very high at the coarse scales we're looking at. They're mostly measured at a fine scale.


Bioclims 6 and 7 are both correlated with bio 13, and bioclim 6 is correlated with DHI var, so I'm removing them. 
```{r}
climate_vars <- c("bio_2", "bio_5", "bio_6", "bio_7", "bio_13", "bio_14", "bio_15")
habitat_vars <- c("_cum", "_var", "_variance", "std_dev")
terrain_vars <- c("elevation_median", "elevation_sd")
stability_vars <- c("stability")
human_vars <- c("gHM")

all_vars <- c(climate_vars, 
              habitat_vars, 
              terrain_vars, 
              stability_vars, 
              human_vars) %>% 
  paste(collapse = "|")

all_vars_full <- df_corr$rowname[str_detect(df_corr$rowname, all_vars)]

corr_reduced <- df_corr %>% 
  select(rowname, any_of(all_vars_full), -contains("coef_of_var"), contains("gHM"), contains("stability")) %>% 
  filter(str_detect(rowname, all_vars), !str_detect(rowname, "coef_of_var"))
```

Get final correlation matrix. Everything looks good!
```{r}
corr_final <- corr_reduced %>% 
  filter(!str_detect(rowname, "bio_6|bio_7")) %>% 
  select(-contains("bio_6"), -contains("bio_7"))

corr_final_vec <- corr_final$rowname

corrr::rplot(corr_final)
```

#### Linear models

Read in data so I don't have to re-run everything previously. Filtering out the point with a Hill number ~0.344 because it's on a tiny island that many of the terrestrial environmental variables don't capture.
```{r}
df_150 <- st_read(here("output", "spreadsheets", "medium_150.geojson"),
                  crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs") %>% 
  filter(hill_1 > 0.35)
corr_final_vec <- read_csv(here("output", "spreadsheets", "keep_nocorr_vars.csv")) %>% 
  pull(1)
```


Helper functions for exploring results
```{r}

# get draws from the posterior distribution
# response can be either hill_1 or sqrt_pi, since those are what we're interested in
sample_response_posterior <- function(model, num_iterations = 1000, response = "hill_1") {
  
  # sample from the posterior distribution
  post_draws <- posterior_predict(model, iter = num_iterations) %>% 
    t() 
  colnames(post_draws) <- paste0("draw_", 1:num_iterations)
  
  d <- model$data
  
  # get the observed response vector
  if (response == "hill_1") {
    resp = d$hill_1
  } else if (response == "sqrt_pi") {
    resp = d$sqrt_pi
  }
  
  # create a data frame of posterior draws and add in the observed values for comparison.
  # in addition, I'm adding in the latitude and longitude 
  post_df <- post_draws %>% 
    as_tibble() %>% 
    mutate(observed = resp,
           longitude = model$data$lon_scaled * 1e6,
           latitude = model$data$lat_scaled * 1e6,
           id = 1:nrow(post_draws)) %>% 
    pivot_longer(cols = c(contains("_"), observed),
               names_to = "draw",
               values_to = "response_post") %>% 
    mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))
  
  return(post_df)
}

# function to retrieve the parameter posteriors and log probability posterior. This will also convert each class of posterior to a data frame with reasonable column names
tidy_post <- function(model) {
  post_dfs <- rstan::extract(model$model, permute = TRUE) %>% 
    map(as_tibble)
  
  # rename beta columns to their variable names
  colnames(post_dfs$B) <- colnames(model$X) %>% 
    janitor::make_clean_names(case = "snake")
  
  # rename spatial effect columns
  suffix <- str_remove(colnames(post_dfs$spatialEffectsKnots), "1.")
  colnames(post_dfs$spatialEffectsKnots) <- paste0("knot_", suffix)
  
  return(post_dfs)
}

```



##### Multiple regression- spatial autocorrelation

Fitting an OLS multiple regression to assess the extent of spatial autocorrelation in the residuals and guide the use of a conditional autoregression.

Fit a multiple regression with the final set of variables.
```{r}
lm_df <- df_150 %>% 
  as_tibble() %>% 
  select(hill_1, any_of(corr_final_vec), -geometry)
  

hill_model_lm <- lm(hill_1 ~ ., data = lm_df)

hill_resid_lm <- hill_model_lm %>% 
  augment() %>% 
  select(.resid)

resid_sf <- df_150 %>% 
  select(hill_1) %>% 
  bind_cols(hill_resid_lm)

ggplot() +
  geom_sf(data = resid_sf, aes(fill = .resid, color = .resid)) +
  scale_fill_viridis_c() +
  scale_color_viridis_c()
```

The residuals don't look like they're spatially autocorrelated, but let's see. Looks to be lower, but let's check out significance.
```{r}
sp_coords_resid <- st_centroid(resid_sf) %>% st_coordinates()
sp_moran_corr <- ncf::spline.correlog(x = sp_coords_resid[,1], 
                                      y = sp_coords_resid[,2], 
                                      z = resid_sf$.resid,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)

plot(sp_moran_corr)
```

Looks like there is significant spatial autocorrelation out to about 1000 km for the full model. 
```{r}
set.seed(928)
resid_correlog_test <- correlog(x = sp_coords_resid[,1], 
                             y = sp_coords_resid[,2], 
                             z = resid_sf$.resid, 
                             increment = 10000, 
                             resamp = 1000, 
                             latlon = FALSE,
                             na.rm = TRUE, 
                             quiet = FALSE)

resid_correlog_df <- tibble(
  distance = resid_correlog_test$mean.of.class,
  pval = resid_correlog_test$p
  ) %>% 
  filter(distance < 1.7e6)



ggplot(data = resid_correlog_df, aes(x = distance, y = pval)) +
  geom_point() +
  geom_hline(yintercept = 0.05, color = "red") + 
  labs(title = paste0("SAC is no longer present at ", round(resid_correlog_test$x.intercept / 1000, 3), " km")) +
  theme_minimal()

```

##### Random Fields GLMM
```{r}
options(mc.cores = parallel::detectCores())
```


Explore priors
```{r}
sigmas <- c(0.2, 0.5, 1, 3, 5, 10)
dfs <- c(3, 5, 10, 100, 1000)

tdist_dfs <- map_dfc(dfs, ~rstudent_t(df = .x, n = 1000, mu = 0, sigma = 1))

colnames(tdist_dfs) <- paste0("df_", dfs)

tdist_dfs <- tdist_dfs %>% 
  pivot_longer(cols = everything(), names_to = "df", values_to = "values")

ggplot(data = tdist_dfs %>% filter(df == "df_100"), aes(x = values, color = df)) +
  geom_density(adjust = 3) +
  scale_fill_viridis_d()
```

Get a data frame with the centroid coordinates of each cell for the regression. I need to scale them down to ~1-10 for the sigma-theta parameter estimation (decay of spatial autocorrelation with distance), which depends on the distance between points
```{r}
df_spatial <- df_150 %>% 
  bind_cols(df_150 %>% 
              st_centroid() %>% 
              st_coordinates() %>% 
              as_tibble()) %>% 
  as_tibble() %>% 
  dplyr::select(-geometry) %>% 
  mutate(lon_scaled = X * 0.000001,
         lat_scaled = Y * 0.000001)
```



###### GDE Full model

###### Influence of knots
Explore the effect of the number of knots on model fitting. NOTE- this code randomly samples seeds for each model. I thought the model object for each model would contain the seed, but it doesn't. To get the model output exactly as I ran it, load the models that I wrote out.
```{r}
model_fits_knots <- map(c(5, 10, 20, 25),
  ~glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = .x,
  iter = 4000,
  save_log_lik = TRUE,
  chains = 2,
  covariance = "exponential",
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 1000,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(1000, 0, 1),
  prior_gp_theta = half_t(1000, 0, 5),
  prior_gp_sigma = half_t(1000, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = sample(1000, 1)
)
)
```

Write models to a file. 
```{r}
model_names <- file.path(here("output", "knot_models"), c("knots_5.rds", "knots_10.rds", "knots_20.rds", "knots_25.rds"))
map2(model_fits_knots, model_names, ~saveRDS(object = .x, file = .y))
```

Read models in if they have already been run.
```{r}
if (!exists("model_fits_knots")) {
  model_names <- file.path(here("output", "knot_models"), c("knots_5.rds", "knots_10.rds", "knots_20.rds", "knots_25.rds"))
  model_fits_knots <- map(model_names, readRDS)
}
```


Use LOO to compare. There are a couple values (2-3 depending on no. knots) that have high pareto-k values. Not too worrisome until I get to the most simple model, though. I think adjusting my priors will fix this. The 20 knot model technically performs the best, but the magnitude and SE of the difference is really too high to say that one performs better than the other. I'll take a look at the residuals and see what those tell me.
```{r}
loo_5 <- loo(model_fits_knots[[1]])
loo_10 <- loo(model_fits_knots[[2]])
loo_20 <- loo(model_fits_knots[[3]])
loo_25 <- loo(model_fits_knots[[4]])

loo_compare <- loo::loo_compare(loo_5, loo_10, loo_20, loo_25)

loo_compare
```

Residual plots for each model. This makes things a bit more clear. It looks like there aren't any trends in the 20 knot model, but there are in the others, especially the 5 and 10 knot models. Let's also look at them spatially!
```{r}
plot(model_fits_knots[[1]], type = "residual-vs-fitted")
plot(model_fits_knots[[2]], type = "residual-vs-fitted")
plot(model_fits_knots[[3]], type = "residual-vs-fitted")
plot(model_fits_knots[[4]], type = "residual-vs-fitted")
```

glmmfields doesn't work with "augment" from the brooms package out of the box, so I'm using a workaround to get the residuals and map them (rather than scatterplot). Doesn't really help illuminate the differences. 
```{r}
resid_plot_5 <- plot(model_fits_knots[[1]], type = "spatial-residual", link = TRUE)
resid_plot_10 <- plot(model_fits_knots[[2]], type = "spatial-residual", link = TRUE)
resid_plot_20 <- plot(model_fits_knots[[3]], type = "spatial-residual", link = TRUE)
resid_plot_25 <- plot(model_fits_knots[[4]], type = "spatial-residual", link = TRUE)

resid_sp_5 <- cbind(df_150, residual = resid_plot_5$data$residual) 

resid_sp_10 <- cbind(df_150, residual = resid_plot_10$data$residual) 

resid_sp_20 <- cbind(df_150, residual = resid_plot_20$data$residual) 

resid_sp_25 <- cbind(df_150, residual = resid_plot_25$data$residual) 

resid_map_5 <- ggplot() +
  geom_sf(data = resid_sp_5, aes(fill = residual, color = residual)) +
  scale_fill_viridis_c(limits = c(-0.13, 0.10), guide = FALSE) +
  scale_color_viridis_c(limits = c(-0.13, 0.10), guide = FALSE) +
  theme_minimal()

resid_map_10 <- ggplot() +
  geom_sf(data = resid_sp_10, aes(fill = residual, color = residual)) +
  scale_fill_viridis_c(limits = c(-0.13, 0.10), guide = FALSE) +
  scale_color_viridis_c(limits = c(-0.13, 0.10), guide = FALSE) +
  theme_minimal()

resid_map_20 <- ggplot() +
  geom_sf(data = resid_sp_20, aes(fill = residual, color = residual)) +
  scale_fill_viridis_c(limits = c(-0.13, 0.10), guide = FALSE) +
  scale_color_viridis_c(limits = c(-0.13, 0.10), guide = FALSE) +
  theme_minimal()

resid_map_25 <- ggplot() +
  geom_sf(data = resid_sp_25, aes(fill = residual, color = residual)) +
  scale_fill_viridis_c(limits = c(-0.13, 0.10)) +
  scale_color_viridis_c(limits = c(-0.13, 0.10)) +
  theme_minimal()

(resid_map_5 + resid_map_10) / (resid_map_20 + resid_map_25)
```
Let's check out spatial autocorrelation in the residuals. It looks like spatial autocorrelation is reduced out to the # knots = 20
```{r}
resid_coords_knots <- st_centroid(resid_sp_5) %>% st_coordinates()


moran_corr_5 <- ncf::spline.correlog(x = resid_coords_knots[,1], 
                                      y = resid_coords_knots[,2], 
                                      z = resid_sp_5$residual,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)

moran_corr_10 <- ncf::spline.correlog(x = resid_coords_knots[,1], 
                                      y = resid_coords_knots[,2], 
                                      z = resid_sp_10$residual,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)

moran_corr_20 <- ncf::spline.correlog(x = resid_coords_knots[,1], 
                                      y = resid_coords_knots[,2], 
                                      z = resid_sp_20$residual,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)

moran_corr_25 <- ncf::spline.correlog(x = resid_coords_knots[,1], 
                                      y = resid_coords_knots[,2], 
                                      z = resid_sp_25$residual,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)

# this is the regular additive multiple regression to compare to
plot(sp_moran_corr, main = "No correction")

plot(moran_corr_5, main = "5 knots")
plot(moran_corr_10, main = "10 knots")
plot(moran_corr_20, main = "20 knots")
plot(moran_corr_25, main = "25 knots")
```

Calculate Moran's I p-values for knots = 20. Woo! Spatial autocorrelation is vanquished. Well, at some distances it pops up again, but I don't think this will affect analyses.
```{r}
set.seed(5)
# calculating the correlation every 10 km (a little overkill, since the cell sizes are 193 km x 193 km)
correlog_test_20 <- correlog(x = resid_coords_knots[,1], 
                             y = resid_coords_knots[,2], 
                             z = resid_sp_20$residual, 
                             increment = 10000, 
                             resamp = 1000, 
                             latlon = FALSE,
                             na.rm = TRUE, 
                             quiet = FALSE)

correlog_df_20 <- tibble(
  distance = correlog_test_20$mean.of.class,
  pval = correlog_test_20$p
  ) %>% 
  filter(distance < 1.7e6)



ggplot(data = correlog_df_20, aes(x = distance, y = pval)) +
  geom_point() +
  geom_hline(yintercept = 0.05, color = "red") + 
  labs(title = paste0("SAC is no longer present at ", round(correlog_test_20$x.intercept / 1000, 3), " km")) +
  theme_minimal()
```


Get posteriors for each model
```{r}
response_posts <- map(model_fits_knots, sample_response_posterior)
param_posts <- map(model_fits_knots, tidy_post)

```

Plot sigma posteriors
```{r}
sigma_post <- bind_cols(param_posts[[1]]$sigma,
                     param_posts[[2]]$sigma) %>% 
  pivot_longer(everything(), names_to = "model", values_to = "sigma")

```

Plot sigma posteriors
```{r}
ggplot(data = sigma_post, aes(x = sigma, y = model)) +
  ggridges::geom_density_ridges()
```


Plot response posteriors
```{r}
response_plots <- map(response_posts, ~ggplot(.x, aes(x = response_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal())

(response_plots[[1]] / response_plots[[2]])
```
Explore divergent transitions
```{r}
var_names <- names(model_fits_knots[[1]]$model)
beta_names <- var_names[str_detect(var_names, "B")]

```


This doesn't render to Rmarkdown (the figure margins are too large). Write to a pdf instead.
```{r, eval=FALSE}
# beta pairs plot
pdf(file = here("output", "exploratory_plots", "beta_pairs_knots_5.pdf"),
    width = 100, height = 100)
pairs(model_fits_knots[[1]]$model, vars = beta_names[1:4])
dev.off()
```


###### Refine priors

Posterior predictive checks implemented in `shinystan` show that the full model captures the center of the data pretty well, but does poorly capturing the standard deviation, minimum, and maximum (too narrow). I think I'm being over-conservative with normal priors, and since I don't have a ton of data the priors are constraining my estimates. I'm going to take advantage of t-distributed priors for capturing the extremes in my data.  

First, I'm going to allow the model to estimate the student-t degrees of freedom parameter. Previously, I used the default, which makes the distribution normal (df = 1000). It's only 1 more parameter to estimate, but it might be difficult given the data.
```{r, df-est}
model_df_est <-
  glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 4000,
  save_log_lik = TRUE,
  chains = 2,
  estimate_df = TRUE,
  covariance = "exponential",
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 1000,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(1000, 0, 1),
  prior_gp_theta = half_t(1000, 0, 5),
  prior_gp_sigma = half_t(1000, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = 2332
)
```


Get the response and posterior sample for posterior predictive checks
```{r}
model_df_est_resp <- model_df_est$data$hill_1
model_df_est_post <- glmmfields::posterior_predict(model_df_est, iter = 1000)
```


The first 2 plots highlight the difference in the observed data from 50 draws from the posterior.  
The last shows boxplots of the distribution of the observed vs 8 draws from the posterior. The center is getting estimated pretty well, but the tails are not.  
```{r}
ppc_dens_overlay(model_df_est_resp, model_df_est_post[1:50,])
ppc_ecdf_overlay(model_df_est_resp, model_df_est_post[1:50,])
ppc_boxplot(model_df_est_resp, model_df_est_post[sample(1000, 8),])
```


Predictive errors are a little too high for my liking.
```{r}
ppc_error_hist(model_df_est_resp, model_df_est_post[1:4,])
```

Further confirmation that the variation in the data isn't being captured by the model. 
First, I did the mean and sd, assuming the responses are normal. There are heavy tails and the posteriors are bimodal though, so I'm also visualizing median and IQR.  
```{r}
ppc_stat_2d(model_df_est_resp, model_df_est_post, stat = c("mean", "sd"))
ppc_stat_2d(model_df_est_resp, model_df_est_post, stat = c("median", "IQR"))
```


Let's also specify a t-distributed intercept prior, rather than a normally distributed prior. The intercept parameter has a lower N_eff than other parameters, so it's possibly being misspecified. I'm going to try a moderate t-distributed prior (df = 50). This gives the distribution a fatter tail without being too outlandish.

```{r, model-intercept}
model_intercept <-
  glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 4000,
  save_log_lik = TRUE,
  chains = 2,
  estimate_df = TRUE,
  covariance = "exponential",
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 50,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(1000, 0, 1),
  prior_gp_theta = half_t(1000, 0, 5),
  prior_gp_sigma = half_t(1000, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = 8338
)
```


Get the response and posterior sample for posterior predictive checks
```{r}
model_intercept_resp <- model_intercept$data$hill_1
model_intercept_post <- glmmfields::posterior_predict(model_intercept, iter = 1000)
```


The first 2 plots highlight the difference in the observed data from 5 draws from the posterior.  
The last shows boxplots of the distribution of the observed vs 8 draws from the posterior. The center is getting estimated pretty well, but the tails are not.  
```{r}
ppc_dens_overlay(model_intercept_resp, model_intercept_post[1:50,])
ppc_ecdf_overlay(model_intercept_resp, model_intercept_post[1:50,])
ppc_boxplot(model_intercept_resp, model_intercept_post[sample(1000, 8),])
```


Predictive errors are a little too high for my liking.
```{r}
ppc_error_hist(model_intercept_resp, model_intercept_post[1:4,])
```

Further confirmation that the variation in the data isn't being captured by the model. 
First, I did the mean and sd, assuming the responses are normal. There are heavy tails and the posteriors are bimodal though, so I'm also visualizing median and IQR.  
```{r}
ppc_stat_2d(model_intercept_resp, model_intercept_post, stat = c("mean", "sd"))
ppc_stat_2d(model_intercept_resp, model_intercept_post, stat = c("median", "IQR"))
```

Looks like all of the same problems as the previous model, with no changes in the sampling of the intercept or its posterior. Since this is the case, I'm changing it back to a normal prior.  

The next thing I'm going to try is the gaussian process error (gp_sigma). Perhaps the error term isn't getting estimated quite right. I'm using a df of 50 to get a reasonably fat tail.  

Update: doesn't change anything.

```{r}
model_gp_sigma <-
  glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 4000,
  save_log_lik = TRUE,
  chains = 2,
  estimate_df = TRUE,
  covariance = "exponential",
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 1000,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(1000, 0, 1),
  prior_gp_theta = half_t(1000, 0, 5),
  prior_gp_sigma = half_t(50, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = 6556
)

```

Get the response and posterior sample for posterior predictive checks
```{r}
model_gp_sigma_resp <- model_gp_sigma$data$hill_1
model_gp_sigma_post <- glmmfields::posterior_predict(model_gp_sigma, iter = 1000)
```

Same shit, different day.
```{r}
ppc_dens_overlay(model_gp_sigma_resp, model_gp_sigma_post[1:50,])
ppc_ecdf_overlay(model_gp_sigma_resp, model_gp_sigma_post[1:50,])
ppc_boxplot(model_gp_sigma_resp, model_gp_sigma_post[sample(1000, 8),])
```

Now I'm going to also allow the gaussian process theta parameter to be t-distributed. df = 50.

```{r}
model_gp_theta <-
  glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 4000,
  save_log_lik = TRUE,
  chains = 2,
  estimate_df = TRUE,
  covariance = "exponential",
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 1000,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(1000, 0, 1),
  prior_gp_theta = half_t(50, 0, 5),
  prior_gp_sigma = half_t(50, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = 7669
)
```




Get the response and posterior sample for posterior predictive checks
```{r}
model_gp_theta_resp <- model_gp_theta$data$hill_1
model_gp_theta_post <- glmmfields::posterior_predict(model_gp_theta, iter = 1000)
```

Same ol' same ol'.
```{r}
ppc_dens_overlay(model_gp_theta_resp, model_gp_theta_post[1:50,])
ppc_ecdf_overlay(model_gp_theta_resp, model_gp_theta_post[1:50,])
ppc_boxplot(model_gp_theta_resp, model_gp_theta_post[sample(1000, 8),])
```


Now going to try a t-distributed error term (sigma). Priors don't seem to be the problem. I'm going to go ahead with model selection using normal priors and inferring the df of the MVT distribution and refine the priors after a simpler model is (possibly) selected.
```{r}
model_sigma <-
  glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 4000,
  save_log_lik = TRUE,
  chains = 2,
  estimate_df = TRUE,
  covariance = "exponential",
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 50,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(50, 0, 0.1),
  prior_sigma = half_t(50, 0, 1),
  prior_gp_theta = half_t(50, 0, 5),
  prior_gp_sigma = half_t(50, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = 1222
)
```

Get the response and posterior sample for posterior predictive checks
```{r}
model_sigma_resp <- model_sigma$data$hill_1
model_sigma_post <- glmmfields::posterior_predict(model_sigma, iter = 1000)
```

Same ol' same ol'.
```{r}
ppc_dens_overlay(model_sigma_resp, model_sigma_post[1:50,])
ppc_ecdf_overlay(model_sigma_resp, model_sigma_post[1:50,])
ppc_boxplot(model_sigma_resp, model_sigma_post[sample(1000, 8),])
```


###### Model selection

Let's try the model selection with a simple multiple regression model followed by incorporating spatial random fields with the final simplified model.

```{r}
model_mult_reg <- stan_glm(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  prior = normal(location = 0, scale = 0.1),
  prior_intercept = normal(location = 0, scale = 1),
  family = gaussian(),
  iter = 4000,
  chains = 2,
  cores = 2
)
```


Get the response and posterior sample for posterior predictive checks
```{r}
model_mult_reg_resp <- model_mult_reg$data$hill_1
model_mult_reg_post <- glmmfields::posterior_predict(model_mult_reg, iter = 1000)
```

Not accounting for spatial autocorrelation results in a posterior that more closely approximates the data. It appears that spatial autocorrelation gives the GDE a more gaussian shape, while the real relationship is likely bimodal, as revealed by the difference in Hill 1 between regions above the freezeline and below it.
```{r}
ppc_dens_overlay(model_mult_reg_resp, model_mult_reg_post[1:50,])
ppc_ecdf_overlay(model_mult_reg_resp, model_mult_reg_post[1:50,])
ppc_boxplot(model_mult_reg_resp, model_mult_reg_post[sample(1000, 8),])
```


```{r}
ppc_stat_2d(model_mult_reg_resp, model_mult_reg_post, stat = c("mean", "sd"))
ppc_stat_2d(model_mult_reg_resp, model_mult_reg_post, stat = c("median", "IQR"))
```


```{r}
# remove the warmup 
mult_reg_posteriors <- as.matrix(model_mult_reg)[2001:4000,]

betas <- colnames(mult_reg_posteriors)[2:10]

mcmc_intervals(mult_reg_posteriors, 
           pars = betas,
           prob = 0.50,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("50% and 90% CI intervals")

mcmc_intervals(mult_reg_posteriors, 
           pars = betas,
           prob = 0.70,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("70% and 90% CI intervals")

```


Definitely unequal variance due to spatial autocorrelation.
```{r}
df_resid <- tibble(resid_mult = residuals(model_mult_reg),
                   fit_mult = fitted(model_mult_reg))


ggplot(data = df_resid, aes(x = fit_mult, y = resid_mult)) + 
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0)
```
Let's try projective variable selection to simplify the model.
```{r}
ref_mult <- get_refmodel(model_mult_reg)

cv_mult <- cv_varsel(ref_mult, method = "forward")

```

It picks the variables we've been seeing to come on top.
```{r}
solution_terms(cv_mult)
```


Makes sense. This is what we've been seeing with other non-spatial approaches.
```{r}
plot(cv_mult, stats = c('elpd', 'rmse'))
```


```{r}
# plot the validation results, this time relative to the full model
plot(cv_mult, stats = c('elpd', 'rmse'), deltas = TRUE)
```


```{r}
 # Visualise the three most relevant variables in the full model -->
 mcmc_areas(as.matrix(ref_mult$fit),
            pars = c(solution_terms(cv_mult)[1:3],
                     "sigma")) 
```

Run a model accounting for spatial autocorrelation with these top variables.
```{r}
model_top_3 <-
  glmmfields(
  hill_1 ~
    current_medium_bio_2 +
    current_medium_bio_5 +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 7000,
  save_log_lik = TRUE,
  chains = 4,
  estimate_df = TRUE,
  covariance = "exponential",
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 1000,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(1000, 0, 1),
  prior_gp_theta = half_t(1000, 0, 5),
  prior_gp_sigma = half_t(1000, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = 3336
)
```


Let's do a quick check to see if the posterior agrees with what I've been seeing. It does!
```{r}
model_top_3_resp <- model_top_3$data$hill_1
model_top_3_post <- glmmfields::posterior_predict(model_top_3, iter = 1000)

ppc_dens_overlay(model_top_3_resp, model_top_3_post[1:50,])
ppc_ecdf_overlay(model_top_3_resp, model_top_3_post[1:50,])
ppc_boxplot(model_top_3_resp, model_top_3_post[sample(1000, 8),])
```


```{r}
# remove the warmup 
top_3_posteriors <- as.matrix(model_top_3$model)[3501:7000,]

betas <- c("B[2]", "B[3]", "B[4]")

mcmc_intervals(top_3_posteriors, 
           pars = betas,
           prob = 0.50,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("50% and 90% CI intervals")

mcmc_intervals(top_3_posteriors, 
           pars = betas,
           prob = 0.70,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("70% and 90% CI intervals")

```


```{r}
ppc_stat_2d(model_top_3_resp, model_top_3_post, stat = c("mean", "sd"))
ppc_stat_2d(model_top_3_resp, model_top_3_post, stat = c("median", "IQR"))
```

Function from Gelman et al. 2018 to calculate a bayes version of the R2 value
```{r}
bayes_R2_glmmfields <- function(fit) {
y_pred <- glmmfields::posterior_linpred(fit)
var_fit <- apply(y_pred, 1, var)
var_res <- as.matrix(fit$model, pars = c("sigma"))^2
var_gp <- as.matrix(fit$model, pars = c("gp_sigma"))^2
return(var_fit / (var_fit + var_res + var_gp))
}

r2_df <- tibble(
  r2_top_3 = bayes_R2_glmmfields(model_top_3)[,1],
  r2_full = bayes_R2_glmmfields(model_full)[,1],
) %>% 
  pivot_longer(cols = everything(),
               names_to = "model",
               values_to = "post")

ggplot(data = r2_df, aes(x = post, color = model)) +
  geom_density()

```

###### GDE New Stability
Let's do the same thing with the new temperature and precipitation stability variables. I previously tried to incorporate the freezing line into the model, hence the object name, but it didn't work- incorporating both spatial autocorrelation and the freezing line division into the model made it too complicated- there were convergence troubles, and bad chain mixing. I think I would need many more observations to justify the increased complexity.  


Read data in and wrangle into an appropriate form.
```{r}
template_medium_rast <- raster(here("data", "templates", "template_medium.tif"))

new_stab_files <- list.files(here("data", "climate_agg"), 
                             pattern = "GlobalExtreme", 
                             full.names = TRUE)

new_stab_rasters <- stack(new_stab_files)

new_stab_sf <- new_stab_rasters %>%  
  projectRaster(template_medium_rast) %>% 
  rasterToPolygons() %>% 
  st_as_sf()

# take the median of all overlapping cells with each of the medium resolution cells
new_stab_df <- st_join(df_150, 
                         new_stab_sf,
                         largest = TRUE)

new_stab_spatial <- new_stab_df %>% 
  bind_cols(new_stab_df %>% 
              st_centroid() %>% 
              st_coordinates() %>% 
              as_tibble()) %>% 
  as_tibble() %>% 
  mutate(temp_trend = normalize(GlobalExtreme_tsTrendExt),
         temp_var = normalize(GlobalExtreme_tsVarExt),
         precip_trend = normalize(GlobalExtreme_prTrendExt),
         precip_var = normalize(GlobalExtreme_prVarExt)) %>% 
  select(-geometry) %>% 
  mutate(lon_scaled = X * 0.000001,
         lat_scaled = Y * 0.000001) %>% 
  select(template_medium, 
         cell, 
         hill_1,
         sqrt_pi,
         any_of(corr_final_vec), 
         temp_trend,
         temp_var,
         precip_trend,
         precip_var,
         min_temp,
         X,
         Y,
         lon_scaled,
         lat_scaled)
```

Write the new data frame and projected new stability layer to file.
```{r, eval=FALSE}
st_write(new_stab_sf, here("data", "climate_poly", "new_stability.geojson"))
write_csv(new_stab_spatial, here("output", "spreadsheets", "model_data.csv"))
```


```{r, eval = FALSE}
set.seed(20934)
model_mult_reg_frz <- stan_glm(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    temp_trend +
    temp_var + 
    precip_trend +
    precip_var,
  data = new_stab_spatial,
  prior = normal(location = 0, scale = 0.1),
  prior_intercept = normal(location = 0, scale = 1),
  family = gaussian(),
  iter = 4000,
  chains = 2,
  cores = 2
)
```

Write the model to file
```{r, eval=FALSE}
write_rds(model_mult_reg_frz, here("output", "models", "mult_reg_new_stab.rds"))
```


```{r, echo=FALSE}
model_mult_reg_frz <- read_rds(here("output", "models", "mult_reg_new_stab.rds"))
```


Get the response and posterior sample for posterior predictive checks
```{r}
model_mult_reg_frz_resp <- model_mult_reg_frz$data$hill_1
model_mult_reg_frz_post <- glmmfields::posterior_predict(model_mult_reg_frz, iter = 1000)
```

Not accounting for spatial autocorrelation results in a posterior that more closely approximates the data. It appears that spatial autocorrelation gives the GDE a more gaussian shape, while the real relationship is likely bimodal, as revealed by the difference in Hill 1 between regions above the freezeline and below it.
```{r}
ppc_dens_overlay(model_mult_reg_frz_resp, model_mult_reg_frz_post[1:50,])
ppc_ecdf_overlay(model_mult_reg_frz_resp, model_mult_reg_frz_post[1:50,])
ppc_boxplot(model_mult_reg_frz_resp, model_mult_reg_frz_post[sample(1000, 8),])
```


```{r}
ppc_stat_2d(model_mult_reg_frz_resp, model_mult_reg_frz_post, stat = c("mean", "sd"))
ppc_stat_2d(model_mult_reg_frz_resp, model_mult_reg_frz_post, stat = c("median", "IQR"))
```


```{r}
# remove the warmup 
mult_reg_frz_posteriors <- as.matrix(model_mult_reg_frz)[2001:4000,]

betas <- colnames(mult_reg_frz_posteriors)[2:13]

mcmc_intervals(mult_reg_frz_posteriors, 
           pars = betas,
           prob = 0.50,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("50% and 90% CI intervals")

mcmc_intervals(mult_reg_frz_posteriors, 
           pars = betas,
           prob = 0.70,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("70% and 90% CI intervals")

```

Let's try projective variable selection.
```{r}
ref_mult_reg_frz <- get_refmodel(model_mult_reg_frz)

cv_mult_reg_frz <- cv_varsel(ref_mult_reg_frz, method = "forward")
```

It picks the variables we've been seeing to come on top.
```{r}
solution_terms(cv_mult_reg_frz)
```


Makes sense. This is what we've been seeing with other non-spatial approaches.
```{r}
plot(cv_mult_reg_frz, stats = c('elpd', 'rmse'))
```


```{r}
# plot the validation results, this time relative to the full model
plot(cv_mult_reg_frz, stats = c('elpd', 'rmse'), deltas = TRUE)
```

It looks like 5 variables will give our model nearly the same amount of performance as the full model without being too large
```{r}
 # Visualise themost relevant variables in the full model -->
 mcmc_areas(as.matrix(ref_mult_reg_frz$fit),
            pars = c(solution_terms(cv_mult_reg_frz)[1:5],
                     "sigma")) 
```


```{r, eval=FALSE}
model_top_5_nc <-
  glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_5 +
    temp_trend +
    temp_var + 
    precip_trend,
  data = new_stab_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 7000,
  save_log_lik = TRUE,
  chains = 4,
  estimate_df = TRUE,
  covariance = "exponential",
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 1000,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(1000, 0, 1),
  prior_gp_theta = half_t(1000, 0, 5),
  prior_gp_sigma = half_t(1000, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = 6666
)


```

Write model to file.
```{r, eval=FALSE}
write_rds(model_top_5_nc, here("output", "models", "glmm_top_5_new_stab.rds"))
```

```{r, echo=FALSE}
model_top_5_nc <- read_rds(here("output", "models", "glmm_top_5_new_stab.rds"))
```


Let's do a quick check to see if the posterior agrees with what I've been seeing. It does!
```{r}
model_top_5_nc_resp <- model_top_5_nc$data$hill_1
model_top_5_nc_post <- glmmfields::posterior_predict(model_top_5_nc, iter = 1000)

ppc_dens_overlay(model_top_5_nc_resp, model_top_5_nc_post[1:50,])
ppc_ecdf_overlay(model_top_5_nc_resp, model_top_5_nc_post[1:50,])
ppc_boxplot(model_top_5_nc_resp, model_top_5_nc_post[sample(1000, 8),])
```

```{r}
plot(model_top_5_nc, type = "residual-vs-fitted")
```


```{r}
# remove the warmup 
top_5_nc_posteriors <- as.matrix(model_top_5_nc$model)[3501:7000,]

betas <- c("B[2]", "B[3]", "B[4]", "B[5]", "B[6]")

mcmc_intervals(top_5_nc_posteriors, 
           pars = betas,
           prob = 0.50,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("50% and 90% CI intervals")

mcmc_intervals(top_5_nc_posteriors, 
           pars = betas,
           prob = 0.70,
           prob_outer = 0.95) +
  geom_vline(xintercept = 0) +
  ggtitle("70% and 95% CI intervals")

```

Let's take a look at this with density plots.
Get posteriors for each model
```{r}
response_posts_top_5_nc <- sample_response_posterior(model_top_5_nc)

param_posts_top_5_nc <- tidy_post(model_top_5_nc)
```

Plot sigma posterior
```{r}
ggplot(data = param_posts_top_5_nc$sigma, aes(x = V1)) +
  geom_density() +
  labs(x = "sigma")
```
Plot beta posteriors
```{r}
hpd_top_5_nc <- param_posts_top_5_nc$B %>% 
  pivot_longer(everything(), names_to = "beta", values_to = "posterior") %>% 
  group_by(beta) %>% 
  summarize(
    hpd_95_hi = HDInterval::hdi(posterior, credMass = 0.95)["upper"],
    hpd_95_low = HDInterval::hdi(posterior, credMass = 0.95)["lower"],
    hpd_90_hi = HDInterval::hdi(posterior, credMass = 0.90)["upper"],
    hpd_90_low = HDInterval::hdi(posterior, credMass = 0.90)["lower"],
    hpd_80_hi = HDInterval::hdi(posterior, credMass = 0.80)["upper"],
    hpd_80_low = HDInterval::hdi(posterior, credMass = 0.80)["lower"],
    hpd_70_hi = HDInterval::hdi(posterior, credMass = 0.70)["upper"],
    hpd_70_low = HDInterval::hdi(posterior, credMass = 0.70)["lower"],
    hpd_50_hi = HDInterval::hdi(posterior, credMass = 0.50)["upper"],
    hpd_50_low = HDInterval::hdi(posterior, credMass = 0.50)["lower"]
    )


param_posts_top_5_nc$B %>% 
  select(-intercept) %>% 
  pivot_longer(everything(), 
               names_to = "parameter",
               values_to = "posterior") %>% 
  ggplot(aes(x = posterior, y = parameter, fill = after_stat(abs(x) > 0))) + 
  stat_halfeye(.width = c(0.50, 0.95, 1)) +
  scale_fill_manual(values = c("gray80", "darkgreen"))
```


Plot response posteriors
```{r}
ggplot(response_posts_top_5_nc, aes(x = response_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()
```



```{r}
ppc_stat_2d(model_top_5_nc_resp, model_top_5_nc_post, stat = c("mean", "sd"))
ppc_stat_2d(model_top_5_nc_resp, model_top_5_nc_post, stat = c("median", "IQR"))
```

Function from Gelman et al. 2018 to calculate a bayes version of the R2 value. I modified it to account for the Gaussian Process variance.
```{r}
bayes_R2_glmmfields <- function(fit) {
y_pred <- glmmfields::posterior_linpred(fit)
var_fit <- apply(y_pred, 1, var)
var_res <- as.matrix(fit$model, pars = c("sigma"))^2
var_gp <- as.matrix(fit$model, pars = c("gp_sigma"))^2
return(var_fit / (var_fit + var_res + var_gp))
}


r2_df_nc <- tibble(
  r2_top_5_nc =  bayes_R2_glmmfields(model_top_5_nc)[,1]
) %>% 
  pivot_longer(cols = everything(),
               names_to = "model",
               values_to = "post")

ggplot(data = r2_df_nc, aes(x = post, color = model)) +
  geom_density()

```


###### GDM Full Model

```{r}
model_gdm_full <- glmmfields(
  sqrt_pi ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    temp_trend +
    temp_var + 
    precip_trend +
    precip_var,
  data = new_stab_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 25,
  iter = 3000,
  save_log_lik = TRUE,
  chains = 4,
  prior_intercept = student_t(
    df = 100,
    location = 0,
    scale = 0.1
  ),
  # betas are going to be very small too (definitely under 0.01), because the predictors are normalized and centered, and the response doesn't go above 0.1). So a sigma of 0.01 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.01),
  prior_sigma = half_t(100, 0, 1),
  prior_gp_theta = half_t(100, 0, 5),
  prior_gp_sigma = half_t(100, 0, 1),
  control = list(adapt_delta = 0.9999,
                 max_treedepth = 15),
  seed = 9142
)

```



###### Model selection

Let's try the model selection with a simple multiple regression model followed by incorporating spatial random fields with the final simplified model.

```{r}
model_mult_reg_gdm <- stan_glm(
  sqrt_pi ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    temp_trend +
    temp_var + 
    precip_trend +
    precip_var,
  data = new_stab_spatial,
  prior = normal(location = 0, scale = 0.1),
  prior_intercept = normal(location = 0, scale = 1),
  family = gaussian(),
  iter = 4000,
  chains = 2,
  cores = 2
)
```


Get the response and posterior sample for posterior predictive checks
```{r}
model_mult_reg_gdm_resp <- model_mult_reg_gdm$data$sqrt_pi
model_mult_reg_gdm_post <- glmmfields::posterior_predict(model_mult_reg_gdm, iter = 1000)
```

Not accounting for spatial autocorrelation results in a posterior that more closely approximates the data. It appears that spatial autocorrelation gives the GDE a more gaussian shape, while the real relationship is likely bimodal, as revealed by the difference in Hill 1 between regions above the freezeline and below it.
```{r}
ppc_dens_overlay(model_mult_reg_gdm_resp, model_mult_reg_gdm_post[1:50,])
ppc_ecdf_overlay(model_mult_reg_gdm_resp, model_mult_reg_gdm_post[1:50,])
ppc_boxplot(model_mult_reg_gdm_resp, model_mult_reg_gdm_post[sample(1000, 8),])
```


```{r}
ppc_stat_2d(model_mult_reg_gdm_resp, model_mult_reg_gdm_post, stat = c("mean", "sd"))
ppc_stat_2d(model_mult_reg_gdm_resp, model_mult_reg_gdm_post, stat = c("median", "IQR"))
```


```{r}
# remove the warmup 
mult_reg_gdm_posteriors <- as.matrix(model_mult_reg_gdm)[2001:4000,]

betas <- colnames(mult_reg_gdm_posteriors)[2:12]

mcmc_intervals(mult_reg_gdm_posteriors, 
           pars = betas,
           prob = 0.50,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("50% and 90% CI intervals")

mcmc_intervals(mult_reg_gdm_posteriors, 
           pars = betas,
           prob = 0.70,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("70% and 90% CI intervals")

```


Definitely unequal variance due to spatial autocorrelation.
```{r}
df_resid <- tibble(resid_mult = residuals(model_mult_reg_gdm),
                   fit_mult = fitted(model_mult_reg_gdm))


ggplot(data = df_resid, aes(x = fit_mult, y = resid_mult)) + 
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0)
```


Let's try projective variable selection.
```{r}
ref_mult_gdm <- get_refmodel(model_mult_reg_gdm)

cv_mult_gdm <- cv_varsel(ref_mult_gdm, method = "forward")

```

Interesting! Precipitation seems to be coming out towards the top too. Unique of GDE
```{r}
solution_terms(cv_mult_gdm)
```


Looks like 3 variables actually performs better than including more variables
```{r}
plot(cv_mult_gdm, stats = c('elpd', 'rmse'))
```


```{r}
# plot the validation results, this time relative to the full model
plot(cv_mult_gdm, stats = c('elpd', 'rmse'), deltas = TRUE)
```


```{r}
 # Visualise the three most relevant variables in the full model -->
 mcmc_areas(as.matrix(ref_mult_gdm$fit),
            pars = c(solution_terms(cv_mult_gdm)[1:3],
                     "sigma")) 
```

```{r}
model_top_3_gdm <-
  glmmfields(
  sqrt_pi ~
    current_medium_bio_5 +
    current_medium_bio_15 +
    precip_trend,
  data = new_stab_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 7000,
  save_log_lik = TRUE,
  chains = 4,
  estimate_df = TRUE,
  covariance = "exponential",
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 1000,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(1000, 0, 1),
  prior_gp_theta = half_t(1000, 0, 5),
  prior_gp_sigma = half_t(1000, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = 2000
)


```

Check residuals
```{r}
plot(model_top_3_gdm, type = "residual-vs-fitted")
```



Let's do a quick check to see if the posterior agrees with what I've been seeing. It does!
```{r}
model_top_3_gdm_resp <- model_top_3_gdm$data$sqrt_pi
model_top_3_gdm_post <- glmmfields::posterior_predict(model_top_3_gdm, iter = 1000)

ppc_dens_overlay(model_top_3_gdm_resp, model_top_3_gdm_post[1:50,])
ppc_ecdf_overlay(model_top_3_gdm_resp, model_top_3_gdm_post[1:50,])
ppc_boxplot(model_top_3_gdm_resp, model_top_3_gdm_post[sample(1000, 8),])
```



```{r}
resid_plot_top_3_gdm <- plot(model_top_3_gdm[[2]], type = "spatial-residual", link = TRUE)

resid_sp_top_3_gdm <- cbind(df_150, residual = resid_plot_top_3_gdm$data$residual) 

resid_coords_top_3_gdm <- st_centroid(resid_sp_top_3_gdm) %>% st_coordinates()


moran_corr_top_3_gdm <- ncf::spline.correlog(x = resid_coords_top_3_gdm[,1], 
                                      y = resid_coords_top_3_gdm[,2], 
                                      z = resid_sp_top_3_gdm$residual,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)
plot(moran_corr_top_3_gdm)
```
The spatial autocorrelation popping up is about the size of a  (<225 km)
```{r}
set.seed(25)
# calculating the correlation every 10 km (a little overkill, since the cell sizes are 193 km x 193 km)
correlog_test_top_3_gdm <- correlog(x = resid_coords_top_3_gdm[,1], 
                             y = resid_coords_top_3_gdm[,2], 
                             z = resid_sp_top_3_gdm$residual, 
                             increment = 10000, 
                             resamp = 1000, 
                             latlon = FALSE,
                             na.rm = TRUE, 
                             quiet = FALSE)

correlog_top_3_gdm_df <- tibble(
  distance = correlog_test_top_3_gdm$mean.of.class,
  pval = correlog_test_top_3_gdm$p
  ) %>% 
  filter(distance < 1.7e6)



ggplot(data = correlog_top_3_gdm_df, aes(x = distance, y = pval)) +
  geom_point() +
  geom_hline(yintercept = 0.05, color = "red") 
  theme_minimal()

```





```{r}
# remove the warmup 
top_3_gdm_posteriors <- as.matrix(model_top_3_gdm$model)[3501:7000,]

betas <- c("B[2]", "B[3]", "B[4]")

mcmc_intervals(top_3_gdm_posteriors, 
           pars = betas,
           prob = 0.50,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("50% and 90% CI intervals")

mcmc_intervals(top_3_gdm_posteriors, 
           pars = betas,
           prob = 0.70,
           prob_outer = 0.95) +
  geom_vline(xintercept = 0) +
  ggtitle("70% and 95% CI intervals")

```


```{r}
ppc_stat_2d(model_top_3_gdm_resp, model_top_3_gdm_post, stat = c("mean", "sd"))
ppc_stat_2d(model_top_3_gdm_resp, model_top_3_gdm_post, stat = c("median", "IQR"))
```

Function from Gelman et al. 2018 to calculate a bayes version of the R2 value
```{r}

r2_df_gdm <- tibble(
  r2_top_3_gdm =  bayes_R2_glmmfields(model_top_3_gdm)[,1]
) %>% 
  pivot_longer(cols = everything(),
               names_to = "model",
               values_to = "post")

ggplot(data = r2_df_gdm, aes(x = post, color = model)) +
  geom_density() +
  ggtitle(paste0("Median R2 = ", round(median(r2_df_gdm$post), 3)))

```













```{r}

full_gdm_post_sum <- tidy(model_gdm_full, conf.int = TRUE, conf.method = "HPDinterval") 
  
full_gdm_post_beta <- full_gdm_post_sum %>% 
  filter(str_detect(term, "B"),
         !str_detect(term, "[1]")) %>% 
  mutate(significance = if_else(
    conf.low <= 0 & conf.high >= 0, 0.6, 1.0
  ))

ggplot(data = full_gdm_post_beta, aes(x = term, alpha = significance)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_alpha(guide = "none") +
  
  coord_flip() +
  theme_minimal()
```

Posterior density. 100 draws from the posterior prediction distribution overlaid with the observed sqrt_pi.
```{r}
post_draws_gdm_full <- posterior_predict(model_gdm_full, iter = 100) %>% 
  t()
colnames(post_draws_gdm_full) <- paste0("draw_", 1:100)

post_df_gdm_full <- post_draws_gdm_full %>% 
  as_tibble() %>% 
  mutate(observed = model_gdm_full$data$sqrt_pi) %>% 
  pivot_longer(cols = everything(),
               names_to = "draw",
               values_to = "sqrt_pi_post") %>% 
  mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))

ggplot(data = post_df_gdm_full, aes(x = sqrt_pi_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()

```

```{r}
post_gdm_full <- rstan::extract(model_gdm_full$model, permute = TRUE)

post_beta_gdm_full <- post_gdm_full$B

# colnames(post_beta_gdm_full) <- c(
#   "intercept",
#   "Prec. Wettest Month",
#     "Prec. Driest Month",
#     "Prec. Seasonality",
#     "Temp. Diurnal Range",
#     "Max. Temp Warmest Month",
#     "Precip. Stability",
#     "Temp. Stability"
# )
post_beta_df_gdm_full <- as_tibble(post_beta_gdm_full)

post_beta_sample_gdm_full <- sample_n(post_beta_df_gdm_full, 1000)


ggplot(data = model_gdm_full$data, 
             aes(x = current_medium_bio_5, y = sqrt_pi)) +
  geom_point() +
  geom_abline(data = post_beta_sample_gdm_full, 
              aes(intercept = V1, 
                  slope = V6), 
              alpha = 0.05,
              color = "darkgrey") +
  geom_abline(intercept = full_gdm_post_sum %>% 
                filter(term == "B[1]") %>% 
                pull(estimate),
              slope = full_gdm_post_sum %>% 
                filter(term == "B[6]") %>% 
                pull(estimate),
              color = "darkgreen") +
  labs(x = "Max. Temp. of Warmest Month",
       y = expression(sqrt(GDM))) +
  theme_minimal()

```














###### Matern model
```{r}
model_matern <-
  glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 4000,
  save_log_lik = TRUE,
  chains = 2,
  estimate_df = TRUE,
  covariance = "matern",
  matern_kappa = 2.5,
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 1000,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(1000, 0, 1),
  prior_gp_theta = half_t(1000, 0, 5),
  prior_gp_sigma = half_t(1000, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = 31002
)
```



Model check
```{r}
model_matern_resp <- model_matern$data$hill_1
model_matern_post <- glmmfields::posterior_predict(model_matern, iter = 1000)

ppc_dens_overlay(model_matern_resp, model_matern_post[1:50,])
ppc_ecdf_overlay(model_matern_resp, model_matern_post[1:50,])
ppc_boxplot(model_matern_resp, model_matern_post[sample(1000, 8),])
```

```{r}
# remove the warmup 
matern_posteriors <- as.matrix(model_matern$model)[2001:4000,]

betas <- c("B[2]", "B[3]", "B[4]", "B[5]", "B[6]", "B[7]", "B[8]", "B[9]", "B[10]")

mcmc_intervals(matern_posteriors, 
           pars = betas,
           prob = 0.50,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("50% and 90% CI intervals")

mcmc_intervals(matern_posteriors, 
           pars = betas,
           prob = 0.70,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("70% and 90% CI intervals")

```

```{r}
plot(model_matern, type = "residual-vs-fitted")
```

```{r}
resid_plot_matern <- plot(model_matern, type = "spatial-residual", link = TRUE)

resid_sp_matern <- cbind(df_150, residual = resid_plot_matern$data$residual) 

resid_coords_matern <- st_centroid(resid_sp_matern) %>% st_coordinates()


moran_corr_matern <- ncf::spline.correlog(x = resid_coords_matern[,1], 
                                      y = resid_coords_matern[,2], 
                                      z = resid_sp_matern$residual,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)
plot(moran_corr_matern)
```



###### Squared exponential model

Like previously, lots of divergent transitions
```{r}
model_sq <-
  glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    ghh_medium_std_dev +
    human_medium_gHM +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 4000,
  save_log_lik = TRUE,
  chains = 2,
  estimate_df = TRUE,
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 1000,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(1000, 0, 1),
  prior_gp_theta = half_t(1000, 0, 5),
  prior_gp_sigma = half_t(1000, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = 31002
)
```





Model check
```{r}
model_sq_resp <- model_sq$data$hill_1
model_sq_post <- glmmfields::posterior_predict(model_sq, iter = 1000)

ppc_dens_overlay(model_sq_resp, model_sq_post[1:50,])
ppc_ecdf_overlay(model_sq_resp, model_sq_post[1:50,])
ppc_boxplot(model_sq_resp, model_sq_post[sample(1000, 8),])
```

```{r}
# remove the warmup 
sq_posteriors <- as.matrix(model_sq$model)[2001:4000,]

betas <- c("B[2]", "B[3]", "B[4]", "B[5]", "B[6]", "B[7]", "B[8]", "B[9]", "B[10]")

mcmc_intervals(sq_posteriors, 
           pars = betas,
           prob = 0.50,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("50% and 90% CI intervals")

mcmc_intervals(sq_posteriors, 
           pars = betas,
           prob = 0.70,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("70% and 90% CI intervals")

```

```{r}
plot(model_sq, type = "residual-vs-fitted")
```

```{r}
resid_plot_sq <- plot(model_sq, type = "spatial-residual", link = TRUE)

resid_sp_sq <- cbind(df_150, residual = resid_plot_sq$data$residual) 

resid_coords_sq <- st_centroid(resid_sp_sq) %>% st_coordinates()


moran_corr_sq <- ncf::spline.correlog(x = resid_coords_sq[,1], 
                                      y = resid_coords_sq[,2], 
                                      z = resid_sp_sq$residual,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)
plot(moran_corr_sq)
```


###### Quick map for Ana
Global map of genetic diversity
```{r}
predictors <- colnames(as.matrix(model_top_3))[2:10]
global_predictors <- as.data.frame(rasters_full_medium,
                                   xy = TRUE,
                                   centroids = TRUE,
                                   na.rm = TRUE) %>% 
  select(predictors, x, y)

global_predictors <- global_predictors %>% 
  mutate(cell = rownames(global_predictors),
         lon_scaled = x * 0.000001,
         lat_scaled = y * 0.000001)


original_df <- all_dfs[2,]$df[[1]] %>% 
  select(predictors)

original_mean <- original_df %>% 
  summarize_all(mean)

original_sd <- original_df %>% 
  summarize_all(sd)

predictors_norm <- global_predictors %>% 
  mutate(
    current_medium_bio_13 = (current_medium_bio_13 - original_mean$current_medium_bio_13) / original_sd$current_medium_bio_13,
    current_medium_bio_14 = (current_medium_bio_14 - original_mean$current_medium_bio_14) / original_sd$current_medium_bio_14,
    current_medium_bio_15 = (current_medium_bio_15 - original_mean$current_medium_bio_15) / original_sd$current_medium_bio_15,
    current_medium_bio_2 = (current_medium_bio_2 - original_mean$current_medium_bio_2) / original_sd$current_medium_bio_2,
    current_medium_bio_5 = (current_medium_bio_5 - original_mean$current_medium_bio_5) / original_sd$current_medium_bio_5,
    ghh_medium_std_dev = (ghh_medium_std_dev - original_mean$ghh_medium_std_dev) / original_sd$ghh_medium_std_dev,
    human_medium_gHM = (human_medium_gHM - original_mean$human_medium_gHM) / original_sd$human_medium_gHM,
    stability_precip_medium = (stability_precip_medium - original_mean$stability_precip_medium) / original_sd$stability_precip_medium,
    stability_temp_medium = (stability_temp_medium - original_mean$stability_temp_medium) / original_sd$stability_temp_medium,
  ) 

global_prediction <- predict(model_mult_reg, 
                             newdata = predictors_norm, 
                             estimate_method = "median",
                             conf_level = 0.95,
                             interval = "confidence")

map_df <- bind_cols(predictors_norm, 
                    global_prediction) %>% 
  mutate(estimate = ...15)
```



```{r}
template_medium <- raster(here("data", "templates", "template_medium.tif")) %>% 
  rasterToPolygons(na.rm = FALSE) %>% 
  st_as_sf(crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs")

map_sf <- map_df %>% 
  st_as_sf(coords = c("x", "y"),
           crs ="+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs") 

map_sf_poly <- st_join(template_medium, 
                       map_sf, 
                       join = st_covers)
```

```{r}
ggplot() +
  geom_sf(data = map_sf_poly %>% filter(estimate < 0.65), aes(fill = estimate, color = estimate)) +
  scale_fill_gradientn(colors = pal) +
  scale_color_gradientn(colors = pal, guide = NULL) + 
  geom_sf(data = world_base_map, fill = "transparent") +
  labs(fill = "GDE") +
  theme_minimal() +
  labs(title = "Global map of genetic diversity evenness",
       caption = paste(str_wrap("This is a preliminary model and predictions are likely to change. In general, higher GDE = more diverse community. Some areas are missing due to missing data in the predictors that is currently being resolved."), collapse = "\n"))
  
```



###### GDE current new stability


```{r}
template_medium_rast <- raster(here("data", "templates", "template_medium.tif"))

new_stab_files <- list.files("~/Downloads/data/past_climate", full.names = TRUE)

new_stab_rasters <- stack(new_stab_files)

new_stab_sf <- new_stab_rasters %>%  
  projectRaster(template_medium_rast) %>% 
  rasterToPolygons() %>% 
  st_as_sf()

# take the median of all overlapping cells with each of the medium resolution cells
new_stab_df <- st_join(df_150, 
                         new_stab_sf,
                         largest = TRUE)

new_stab_spatial <- new_stab_df %>% 
  bind_cols(new_stab_df %>% 
              st_centroid() %>% 
              st_coordinates() %>% 
              as_tibble()) %>% 
  as_tibble() %>% 
  mutate(temp_trend = normalize(GlobalExtreme_tsTrendExt),
         temp_var = normalize(GlobalExtreme_tsVarExt),
         precip_trend = normalize(GlobalExtreme_prTrendExt),
         precip_var = normalize(GlobalExtreme_prVarExt)) %>% 
  select(-geometry) %>% 
  mutate(lon_scaled = X * 0.000001,
         lat_scaled = Y * 0.000001)
```

```{r}
ggplot(data = new_stab_spatial, aes(x = precip_trend, y = current_medium_bio_15)) +
  geom_point()
```



```{r}
model_cns <- glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    # ghh_medium_std_dev +
    # human_medium_gHM +
    stability_temp_medium +
    stability_precip_medium,
  data = new_stab_spatial,
  family = gaussian(),
  estimate_df = TRUE,
  covariance = "exponential",
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 20,
  iter = 4000,
  chains = 2,
  save_log_lik = TRUE,
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 1000,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(1000, 0, 1),
  prior_gp_theta = half_t(1000, 0, 5),
  prior_gp_sigma = half_t(1000, 0, 1),
  control = list(adapt_delta = 0.99,
                 max_treedepth = 15),
  seed = 13555
)
```
Model check
```{r}
model_cns_resp <- model_cns$data$hill_1
model_cns_post <- glmmfields::posterior_predict(model_cns, iter = 1000)

ppc_dens_overlay(model_cns_resp, model_cns_post[1:50,])
ppc_ecdf_overlay(model_cns_resp, model_cns_post[1:50,])
ppc_boxplot(model_cns_resp, model_cns_post[sample(1000, 8),])
```

```{r}
# remove the warmup 
cns_posteriors <- as.matrix(model_cns$model)[2001:4000,]

betas <- c("B[2]", "B[3]", "B[4]", "B[5]", "B[6]", "B[7]", "B[8]")

mcmc_intervals(cns_posteriors, 
           pars = betas,
           prob = 0.50,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("50% and 90% CI intervals")

mcmc_intervals(cns_posteriors, 
           pars = betas,
           prob = 0.70,
           prob_outer = 0.90) +
  geom_vline(xintercept = 0) +
  ggtitle("70% and 90% CI intervals")

```

```{r}
plot(model_cns, type = "residual-vs-fitted")
```

```{r}
resid_plot_cns <- plot(model_cns, type = "spatial-residual", link = TRUE)

resid_sp_cns <- cbind(df_150, residual = resid_plot_cns$data$residual) 

resid_coords_cns <- st_centroid(resid_sp_cns) %>% st_coordinates()


moran_corr_cns <- ncf::spline.correlog(x = resid_coords_cns[,1], 
                                      y = resid_coords_cns[,2], 
                                      z = resid_sp_cns$residual,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)
plot(moran_corr_cns)
```



###### GDE Current + Stability

```{r}

model_cs <- glmmfields(
  hill_1 ~
    current_medium_bio_13 +
    current_medium_bio_14 +
    current_medium_bio_15 +
    current_medium_bio_2 +
    current_medium_bio_5 +
    stability_precip_medium +
    stability_temp_medium,
  data = df_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 25,
  iter = 7000,
  chains = 4,
  save_log_lik = TRUE,
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 100,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(100, 0, 1),
  prior_gp_theta = half_t(100, 0, 5),
  prior_gp_sigma = half_t(100, 0, 1),
  control = list(adapt_delta = 0.9999,
                 max_treedepth = 15),
  seed = 127
)
```

```{r}
model_cs
```

Posterior density. 100 draws from the posterior prediction distribution overlaid with the observed Hill 1.
```{r}
post_draws <- posterior_predict(model_cs, iter = 100) %>% 
  t()
colnames(post_draws) <- paste0("draw_", 1:100)

post_df <- post_draws %>% 
  as_tibble() %>% 
  mutate(observed = model_cs$data$hill_1) %>% 
  pivot_longer(cols = everything(),
               names_to = "draw",
               values_to = "hill_1_post") %>% 
  mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))

ggplot(data = post_df, aes(x = hill_1_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()

```


```{r}
cs_post_sum <- tidy(model_cs, conf.int = TRUE, conf.method = "HPDinterval") 
  
cs_post_beta <- cs_post_sum %>% 
  filter(str_detect(term, "B"),
         !str_detect(term, "[1]")) %>% 
  mutate(significance = if_else(
    conf.low <= 0 & conf.high >= 0, 0.6, 1.0
  ))

ggplot(data = cs_post_beta, aes(x = term, alpha = significance)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_alpha(guide = "none") +
  
  coord_flip() +
  theme_minimal()
```

Beta posteriors
```{r}
post_cs <- rstan::extract(model_cs$model, permute = TRUE)

post_beta <- post_cs$B

colnames(post_beta) <- c(
  "intercept",
  "Prec. Wettest Month",
    "Prec. Driest Month",
    "Prec. Seasonality",
    "Temp. Diurnal Range",
    "Max. Temp Warmest Month",
    "Precip. Stability",
    "Temp. Stability"
)
post_beta_df <- as_tibble(post_beta)

post_beta_long <- post_beta_df %>% 
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "posterior") %>% 
  mutate(significance = case_when(
    variable == "Max. Temp Warmest Month" ~ 1.0,
    #variable == "Temp. Diurnal Range" ~ 1.0,
    variable == "Temp. Stability" ~ 1.0,
    TRUE ~ 0.0
  ))



post_beta_plot <- post_beta_long %>% 
  filter(variable != "intercept") %>% 
  ggplot(aes(x = posterior, y = variable, alpha = significance)) +
  ggridges::geom_density_ridges() +
  scale_alpha(guide = "none") +
  labs(x = "Effect size", y = "Predictor") +
  geom_vline(xintercept = 0, color = "darkgreen") + 
  theme_minimal() +
  theme(axis.title.y = element_blank())

post_beta_plot
```


###### GDE Temp
```{r}
temp_trend_spatial <- temp_trend_df %>% 
  bind_cols(temp_trend_df %>% 
              st_centroid() %>% 
              st_coordinates() %>% 
              as_tibble()) %>% 
  as_tibble() %>% 
  mutate(GlobalExtreme_tsTrendExt = normalize(GlobalExtreme_tsTrendExt)) %>% 
  select(-geometry) %>% 
  mutate(lon_scaled = X * 0.000001,
         lat_scaled = Y * 0.000001)
  

model_t <- glmmfields(
  hill_1 ~
    current_medium_bio_2 +
    current_medium_bio_5 +
    GlobalExtreme_tsTrendExt,
  data = temp_trend_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 25,
  iter = 3000,
  chains = 4,
  save_log_lik = TRUE,
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 100,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.1),
  prior_sigma = half_t(100, 0, 1),
  prior_gp_theta = half_t(100, 0, 5),
  prior_gp_sigma = half_t(100, 0, 1),
  control = list(adapt_delta = 0.9999,
                 max_treedepth = 15),
  seed = 53699
)
```

```{r}
model_t
```

Posterior density. 100 draws from the posterior prediction distribution overlaid with the observed Hill 1.
```{r}
post_draws <- posterior_predict(model_t, iter = 100) %>% 
  t()
colnames(post_draws) <- paste0("draw_", 1:100)

post_df <- post_draws %>% 
  as_tibble() %>% 
  mutate(observed = model_t$data$hill_1) %>% 
  pivot_longer(cols = everything(),
               names_to = "draw",
               values_to = "hill_1_post") %>% 
  mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))

ggplot(data = post_df, aes(x = hill_1_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()

```


```{r}

t_post_sum <- tidy(model_t, conf.int = TRUE, conf.method = "HPDinterval") 
  
t_post_beta <- t_post_sum %>% 
  filter(str_detect(term, "B"),
         !str_detect(term, "[1]")) %>% 
  mutate(significance = if_else(
    conf.low <= 0 & conf.high >= 0, 0.6, 1.0
  ))

ggplot(data = t_post_beta, aes(x = term, alpha = significance)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_alpha(guide = "none") +
  
  coord_flip() +
  theme_minimal()
```

###### GDE Posterior plots
Posterior density. 100 draws from the posterior prediction distribution overlaid with the observed Hill 1.
```{r}
post_draws <- posterior_predict(model_t, iter = 100) %>% 
  t()
colnames(post_draws) <- paste0("draw_", 1:100)

post_df <- post_draws %>% 
  as_tibble() %>% 
  mutate(observed = model_t$data$hill_1) %>% 
  pivot_longer(cols = everything(),
               names_to = "draw",
               values_to = "hill_1_post") %>% 
  mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))

ggplot(data = post_df, aes(x = hill_1_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()

```


B2 = current_medium_bio_13, B3 = current_medium_bio_14, B4 = current_medium_bio_15, B5 = current_medium_bio_2, B6 = current_medium_bio_5, B7 = ghh_medium_std_dev, B8 = stability_temp_medium

```{r}

rf_post_sum <- tidy(model_rf, conf.int = TRUE, conf.method = "HPDinterval") 
  
rf_post_beta <- rf_post_sum %>% 
  filter(str_detect(term, "B"),
         !str_detect(term, "[1]")) %>% 
  mutate(significance = if_else(
    conf.low <= 0 & conf.high >= 0, 0.6, 1.0
  ))

ggplot(data = rf_post_beta, aes(x = term, alpha = significance)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_alpha(guide = "none") +
  
  coord_flip() +
  theme_minimal()
```

Beta posteriors
```{r}
post_rf <- rstan::extract(model_rf$model, permute = TRUE)

post_beta <- post_rf$B

colnames(post_beta) <- c(
  "intercept",
  "Prec. Wettest Month",
    "Prec. Driest Month",
    "Prec. Seasonality",
    "Temp. Diurnal Range",
    "Max. Temp Warmest Month",
    "Habitat Stand. Dev.",
    "Temp. Stability"
)

post_beta_df <- as_tibble(post_beta)

post_beta_long <- post_beta_df %>% 
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "posterior") %>% 
  mutate(significance = case_when(
    variable == "Max. Temp Warmest Month" ~ 1.0,
    variable == "Temp. Diurnal Range" ~ 1.0,
    variable == "Temp. Stability" ~ 1.0,
    TRUE ~ 0.0
  ))



post_beta_plot <- post_beta_long %>% 
  filter(variable != "intercept") %>% 
  ggplot(aes(x = posterior, y = variable, alpha = significance)) +
  ggridges::geom_density_ridges() +
  scale_alpha(guide = "none") +
  labs(x = "Effect size", y = "Predictor") +
  geom_vline(xintercept = 0, color = "darkgreen") + 
  theme_minimal() +
  theme(axis.title.y = element_blank())

post_beta_plot
```

Intercept posterior
```{r}
post_beta_long %>% 
  filter(variable == "intercept") %>% 
  ggplot(aes(x = posterior)) +
  labs(y = "") +
  geom_density(fill = "grey") +
  labs(x = "Intercept") +
  theme_minimal()
```


```{r}
rf_linpred <- posterior_linpred(model_rf, iter = 100)

rf_linpred_draws %>% 
  ggplot(aes(x = current_medium_bio_5, y = hill_1)) +
  #stat_lineribbon(aes(y = .value)) +
  geom_line(aes(y = .value, group = .draw), alpha = .1) +
  geom_point(data = model_rf$data) +
  scale_fill_brewer(palette = "Greys") 

```

Maximum temperature of the warmest month
```{r}
post_beta_sample <- sample_n(post_beta_df, 1000)

bio_5_plot <- ggplot(data = model_rf$data, 
             aes(x = current_medium_bio_5, y = hill_1)) +
  geom_point() +
  geom_abline(data = post_beta_sample, 
              aes(intercept = intercept, 
                  slope = `Max. Temp Warmest Month`), 
              alpha = 0.05,
              color = "darkgrey") +
  geom_abline(intercept = rf_post_sum %>% 
                filter(term == "B[1]") %>% 
                pull(estimate),
              slope = rf_post_sum %>% 
                filter(term == "B[6]") %>% 
                pull(estimate),
              color = "darkgreen") +
  labs(x = "Maximum temperature of the warmest month",
       y = "Genetic evenness") +
  theme_minimal()

bio_5_plot
```

Temperature mean diurnal range
```{r}
bio_2_plot <- ggplot(data = model_rf$data, 
             aes(x = current_medium_bio_2, y = hill_1)) +
  geom_point() +
  geom_abline(data = post_beta_sample, 
              aes(intercept = intercept, 
                  slope = `Temp. Diurnal Range`), 
              alpha = 0.05,
              color = "darkgrey") +
  geom_abline(intercept = rf_post_sum %>% 
                filter(term == "B[1]") %>% 
                pull(estimate),
              slope = rf_post_sum %>% 
                filter(term == "B[5]") %>% 
                pull(estimate),
              color = "darkgreen") +
  labs(x = "Temperature mean diurnal range",
       y = "Genetic evenness") +
  theme_minimal()

bio_2_plot
```

Temperature stability
```{r}
temp_stability_plot <- ggplot(data = model_rf$data, 
             aes(x = stability_temp_medium, y = hill_1)) +
  geom_point() +
  geom_abline(data = post_beta_sample, 
              aes(intercept = intercept, 
                  slope = `Temp. Stability`), 
              alpha = 0.05,
              color = "darkgrey") +
  geom_abline(intercept = rf_post_sum %>% 
                filter(term == "B[1]") %>% 
                pull(estimate),
              slope = rf_post_sum %>% 
                filter(term == "B[8]") %>% 
                pull(estimate),
              color = "darkgreen") +
  labs(x = "Temperature stability",
       y = "Genetic evenness") +
  theme_minimal()

temp_stability_plot
```

Boxplot of freezing line division
```{r}
freeze_plot <- model_full$data %>% 
  mutate(freezeline = if_else(min_temp == "temperate", "Below", "Above")) %>% 
  ggplot(aes(x = freezeline, y = hill_1)) +
  geom_boxplot() +
  geom_jitter(width = 0.2, alpha = 0.6, color = "darkgreen") +
  labs(x = "Freezeline position",
       y = "Genetic evenness") +
  theme_minimal()

freeze_plot
```

```{r}
(freeze_plot + post_beta_plot) / (bio_5_plot + bio_2_plot + temp_stability_plot) + plot_annotation(tag_levels = 'A')
```

Global map of genetic diversity
```{r}
predictors <- colnames(model_rf$X)[-1]
global_predictors <- as.data.frame(rasters_full_medium,
                                   xy = TRUE,
                                   centroids = TRUE,
                                   na.rm = TRUE) %>% 
  select(predictors, x, y)

global_predictors <- global_predictors %>% 
  mutate(cell = rownames(global_predictors),
         lon_scaled = x * 0.000001,
         lat_scaled = y * 0.000001)


normalize <- function(x) {
  a <- x - mean(x)
  b <- sd(x)
  c <- a / b
  return(c)
}

predictors_norm <- global_predictors %>% 
  mutate_at(predictors, normalize) 

global_prediction <- predict(model_rf, 
                             predictors_norm, 
                             estimate_method = "median",
                             conf_level = 0.95,
                             interval = "confidence")

map_df <- bind_cols(predictors_norm, 
                    global_prediction)
```



```{r}
template_medium <- raster(here("data", "templates", "template_medium.tif")) %>% 
  rasterToPolygons(na.rm = FALSE) %>% 
  st_as_sf(crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs")

map_sf <- map_df %>% 
  st_as_sf(coords = c("x", "y"),
           crs ="+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs") 

map_sf_poly <- st_join(template_medium, 
                       map_sf, 
                       join = st_covers)
```

```{r}
ggplot() +
  geom_sf(data = map_sf_poly %>% filter(estimate < 0.65), aes(fill = estimate, color = estimate)) +
  scale_fill_gradientn(colors = pal) +
  scale_color_gradientn(colors = pal, guide = NULL) + 
  geom_sf(data = world_base_map, fill = "transparent") +
  labs(fill = "GDE") +
  theme_minimal()
```



```{r}
plot(model_rf, type = "spatial-residual", link = TRUE) +
  geom_point(size = 3)
```

```{r}
plot(model_rf, type = "residual-vs-fitted")
```
```{r}
plot(model_rf, type = "prediction", link = FALSE) +
  viridis::scale_colour_viridis() +
  geom_point(size = 3)
```

```{r}
tidy(model_rf, conf.int = TRUE, conf.method = "HPDinterval")
```

###### GDE Freeze line

```{r}
model_t <- glmmfields(
  hill_1 ~
    min_temp,
  data = temp_trend_spatial,
  family = gaussian(),
  lat = "lat_scaled",
  lon = "lon_scaled",
  nknots = 25,
  iter = 3000,
  chains = 2,
  save_log_lik = TRUE,
  # intercept can't move beyond -1 or 1, so a relatively small scale is justified.
  prior_intercept = student_t(
    df = 100,
    location = 0,
    scale = 1
  ),
  # betas are going to be very small too (definitely under 0.05), because the predictors are normalized and centered, and the response is bounded between zero and one (our data is between 0.35ish and 0.65ish, and Hill numbers wouldn't realistically reach the extremes). So a sigma of 0.1 is weakly regularizing and a normal prior is appropriate. 
  prior_beta = student_t(1000, 0, 0.5),
  prior_sigma = half_t(100, 0, 1),
  prior_gp_theta = half_t(100, 0, 5),
  prior_gp_sigma = half_t(100, 0, 1),
  control = list(adapt_delta = 0.9999,
                 max_treedepth = 15),
  seed = 5369
)
```
```{r}
model_t
```

```{r}
post_draws <- posterior_predict(model_t, iter = 500) %>% 
  t()
colnames(post_draws) <- paste0("draw_", 1:500)

post_df <- post_draws %>% 
  as_tibble() %>% 
  mutate(observed = model_t$data$hill_1) %>% 
  pivot_longer(cols = everything(),
               names_to = "draw",
               values_to = "hill_1_post") %>% 
  mutate(post_samples = if_else(draw == "observed", "observed", "posterior"))

ggplot(data = post_df, aes(x = hill_1_post, 
                           group = draw,
                           color = post_samples)) +
  stat_density(geom = "line", alpha = 0.6, position = "identity") +
  scale_color_manual(values = c("darkgreen", "darkgrey")) +
  theme_minimal()

```



```{r}

t_post_sum <- tidy(model_t, conf.int = TRUE, conf.method = "HPDinterval") 
  
t_post_beta <- t_post_sum %>% 
  filter(str_detect(term, "B"),
         !str_detect(term, "[1]")) %>% 
  mutate(significance = if_else(
    conf.low <= 0 & conf.high >= 0, 0.6, 1.0
  ))

ggplot(data = t_post_beta, aes(x = term, alpha = significance)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_alpha(guide = "none") +
  
  coord_flip() +
  theme_minimal()
```



Global map of genetic diversity
```{r}
predictors_gdm_full <- colnames(model_gdm_full$X)[-1]
global_predictors_gdm_full <- as.data.frame(rasters_full_medium,
                                   xy = TRUE,
                                   centroids = TRUE,
                                   na.rm = TRUE) %>% 
  select(predictors_gdm_full, x, y)

global_predictors_gdm_full <- global_predictors_gdm_full %>% 
  mutate(cell = rownames(global_predictors_gdm_full),
         lon_scaled = x * 0.000001,
         lat_scaled = y * 0.000001)


normalize <- function(x) {
  a <- x - mean(x)
  b <- sd(x)
  c <- a / b
  return(c)
}

predictors_gdm_full_norm <- global_predictors_gdm_full %>% 
  mutate_at(predictors_gdm_full, normalize) 

global_prediction_gdm_full <- predict(model_gdm_full, 
                             predictors_gdm_full_norm, 
                             estimate_method = "median",
                             conf_level = 0.95,
                             interval = "confidence")

map_df_gdm_full <- bind_cols(global_predictors_gdm_full, 
                    global_prediction_gdm_full)
```



```{r}
map_sf_gdm_full <- map_df_gdm_full %>% 
  st_as_sf(coords = c("x", "y"),
           crs ="+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs")

map_sf_poly_gdm_full <- st_join(template_medium, 
                       map_sf_gdm_full, 
                       join = st_covers)
```

```{r}
ggplot() +
  geom_sf(data = map_sf_poly_gdm_full, aes(fill = estimate, color = estimate)) +
  scale_fill_gradientn(colors = pal, na.value = "transparent") +
  scale_color_gradientn(colors = pal, na.value = "transparent", guide = NULL) +
  geom_sf(data = world_base_map, fill = "transparent") +
  labs(fill = expression(sqrt(GDM))) +
  theme_minimal()
```


```{r}
ggplot() +
   geom_sf(data = df_150, aes(fill = stability_temp_medium, color = stability_temp_medium)) + 
   scale_fill_gradientn(colors = pal, na.value = "transparent") +
   scale_color_gradientn(colors = pal, na.value = "transparent", guide = NULL) +
   geom_sf(data = world_base_map, fill = "transparent") +
   labs(fill = "Temp. Stability") +
   theme_minimal()
```

New stability
```{r}
template_medium_rast <- raster(here("data", "templates", "template_medium.tif"))

temp_trend <- raster("/Users/connorfrench/Downloads/data/past_climate/GlobalExtreme_tsTrendExt.tif") %>% 
  projectRaster(template_medium_rast) %>% 
  rasterToPolygons() %>% 
  st_as_sf()

# take the median of all overlapping cells with each of the medium resolution cells
temp_trend_df <- st_join(df_150, 
                         temp_trend,
                         largest = TRUE) 

ggplot() +
  geom_sf(data = temp_trend_df, aes(fill = GlobalExtreme_tsTrendExt,
                                 color = GlobalExtreme_tsTrendExt)) +
  scale_fill_gradientn(colors = pal) +
  scale_color_gradientn(colors = pal) +
  theme_minimal()

```

```{r}
ggplot(data = temp_trend_df, aes(x = GlobalExtreme_tsTrendExt, y = hill_1)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Precipitation stability
```{r}
precip_trend <- raster("/Users/connorfrench/Downloads/data/past_climate/GlobalExtreme_prVarExt.tif") %>% 
  projectRaster(template_medium_rast) %>% 
  rasterToPolygons() %>% 
  st_as_sf()

# take the median of all overlapping cells with each of the medium resolution cells
precip_trend_df <- st_join(df_150, 
                         precip_trend,
                         largest = TRUE)

ggplot() +
  geom_sf(data = precip_trend_df, aes(fill = GlobalExtreme_prVarExt,
                                 color = GlobalExtreme_prVarExt)) +
  scale_fill_gradientn(colors = pal) +
  scale_color_gradientn(colors = pal) +
  theme_minimal()
```

```{r}
ggplot(data = precip_trend_df, aes(x = GlobalExtreme_prVarExt, y = hill_1)) +
  geom_point() +
  geom_smooth(method = "lm")
```

```{r}

new_vars <- colnames(new_stab_spatial)

new_vars <- new_vars[str_detect(new_vars, "current|temp_PC|precip_PC|dhi|_trend|_var|stability")]

plot_predictors <- function(predictor){
  ggplot(data = new_stab_spatial, aes_string(x = predictor, y = "hill_1", color = "min_temp")) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_color_viridis_d() +
  theme_minimal()
}

hill_1_plots <- map(new_vars, plot_predictors)

  
```


```{r}
hill_1_plots[[1]] + hill_1_plots[[2]] + hill_1_plots[[3]] + hill_1_plots[[4]]
```



```{r}
hill_1_plots[[5]] + hill_1_plots[[6]] + hill_1_plots[[7]] + hill_1_plots[[8]]
```


```{r}
hill_1_plots[[9]] + hill_1_plots[[10]] + hill_1_plots[[11]] + hill_1_plots[[12]]
```


```{r}
hill_1_plots[[13]] + hill_1_plots[[14]] + hill_1_plots[[15]] + hill_1_plots[[16]]
```

```{r}
hill_1_plots[[17]] + hill_1_plots[[18]] + hill_1_plots[[19]] + hill_1_plots[[20]]
```


```{r}
hill_1_plots[[21]] + hill_1_plots[[22]] + hill_1_plots[[23]] + hill_1_plots[[24]]
```

```{r}
hill_1_plots[[21]] + hill_1_plots[[22]] + hill_1_plots[[23]] + hill_1_plots[[24]]
```

```{r}
hill_1_plots[[25]] + hill_1_plots[[26]] + hill_1_plots[[27]] + hill_1_plots[[28]]
```


```{r}
hill_1_plots[[29]] + hill_1_plots[[30]] + hill_1_plots[[31]] + hill_1_plots[[32]]
```

```{r}
hill_1_plots[[25]] / hill_1_plots[[26]]
```

```{r}
summary(lm(hill_1 ~ temp_PC2 * min_temp + temp_trend * min_temp, data = new_stab_spatial))
```

```{r}
summary(lm(hill_1 ~ current_medium_bio_5 * min_temp + stability_temp_medium * min_temp, data = new_stab_spatial))
```


##### Model comparison

```{r}
loo_cs <- loo(model_cs)
loo_t <- loo(model_t)


loo::loo_compare(loo_cs, loo_t)
```

## Extreme cell resampling

Read in data  

```{r}
pw_pi <- read_csv(here("output", "spreadsheets", "med_3_150_pi.csv"))

analysis_data <- read_csv(here("output", "spreadsheets", "model_data.csv"))

pw_pi_filt <- pw_pi %>% 
  filter(cell %in% analysis_data$cell)

pi_ordered <- pw_pi_filt %>% 
  group_by(order) %>% 
  arrange(desc(pi)) %>% 
  mutate(index = row_number()) %>% 
  ungroup()
```


Resampling functions

```{r}
hill_resample <- function(pi_in) {
  rs_pi <- rerun(1000, sample(pi_in, 150) %>% hill_calc()) %>% 
    unlist()
  return(rs_pi)
}

gdm_resample <- function(pi_in) {
  rs_pi <- rerun(1000, sample(pi_in, 150) %>% mean() %>% sqrt()) %>% 
    unlist()
  return(rs_pi)
}
```

Resampling  

```{r}
dense_cells <- pi_ordered %>% 
  count(cell) %>% 
  slice_max(order_by = n, n = 10) %>% 
  pull(cell)

set.seed(3988)
gde_res_df <- pi_ordered %>% 
  filter(cell %in% dense_cells) %>% 
  group_by(cell) %>% 
  summarize(gde_res = hill_resample(pi_in = pi))

set.seed(877779)
gdm_res_df <- pi_ordered %>% 
  filter(cell %in% dense_cells) %>% 
  group_by(cell) %>% 
  summarize(gdm_res = gdm_resample(pi_in = pi))

```

GDE  

```{r}

gr_gde <- gde_res_df %>% 
  mutate(cell = as.factor(cell)) %>% 
  ggplot(aes(x = gde_res, y = cell)) +
  ggridges::geom_density_ridges(rel_min_height = 0.01)

# Extract the data ggplot used to prepare the figure.
#   purrr::pluck is grabbing the "data" list from the list that
#   ggplot_build creates, and then extracting the first element of that list.
ingredients <- ggplot_build(gr_gde) %>% purrr::pluck("data", 1)

# Pick the highest point. Could easily add quantiles or other features here.
density_lines <- ingredients %>%
  group_by(group) %>% filter(density == max(density)) %>% ungroup()

resample_plot_gde <- gr_gde +
  geom_segment(data = density_lines, 
               aes(x = x, y = ymin, xend = x, 
                   yend = ymin+density*scale*iscale)) +
  scale_x_continuous(limits = c(min(analysis_data$hill_1), max(analysis_data$hill_1))) +
  labs(x = "GDE", title = "1000 resamples of the top 10 most densely sample cells")


resample_plot_gde
```


```{r}
gr_gdm <- gdm_res_df %>% 
  mutate(cell = as.factor(cell)) %>% 
  ggplot(aes(x = gdm_res, y = cell)) +
  ggridges::geom_density_ridges(rel_min_height = 0.01)

# Extract the data ggplot used to prepare the figure.
#   purrr::pluck is grabbing the "data" list from the list that
#   ggplot_build creates, and then extracting the first element of that list.
ingredients <- ggplot_build(gr_gdm) %>% purrr::pluck("data", 1)

# Pick the highest point. Could easily add quantiles or other features here.
density_lines <- ingredients %>%
  group_by(group) %>% filter(density == max(density)) %>% ungroup()

resample_plot_gdm <- gr_gdm +
  geom_segment(data = density_lines, 
               aes(x = x, y = ymin, xend = x, 
                   yend = ymin+density*scale*iscale)) +
  scale_x_continuous(limits = c(min(analysis_data$sqrt_pi), max(analysis_data$sqrt_pi))) +
  labs(x = "GDM", title = "1000 resamples of the top 10 most densely sample cells")


resample_plot_gdm
```

Combo plot  

```{r}
combo_resample <- resample_plot_gdm / resample_plot_gde

combo_resample
```




## Taxonomic exploration

Going to see if there are sampling biases according to which orders were sampled. If so, I'm going to look at potential differences in genetic diversity among the orders that are biased in sampling.  

### Sampling

I want to know whether a particular order was sampled at a particular cell

I want to know how many OTUs of each order are sampled relative to the total number of OTUs.
```{r}
new_stab_spatial <- read_csv(here("output", "spreadsheets", "model_data.csv"))

bold_meta <- read_csv(here("output", "spreadsheets", "bold_sf_pts.csv"))

bold_cell_sum <- read_csv(here("output", "spreadsheets", "bold_cell_summaries.csv")) %>% 
  filter(res_filter == "medium", 
         ind_filter == 3, 
         otu_filter == 150,
         cell %in% new_stab_spatial$cell) %>% 
  left_join(new_stab_spatial, by = "cell")

# for sf geometries
gpi <- read_sf(here("output", "spreadsheets", "global_pred_intervals_gde.geojson")) %>% 
  mutate(cell = as.integer(cell))
  
  
bold_meta_filt <- bold_meta %>% 
  filter(cell_medium %in% bold_cell_sum$cell) %>% 
  select(-cell_high, -cell_low) %>% 
  rename(cell = cell_medium)

```


Let's take a look at whether a particular order was sampled in a cell. The y-axis indicates the cell number, while the x-axis indicates the order. Red cells are present, while cream cells are absent. Cells closer to each other on the y-axis are generally closer to each other geographically, although the distance between cells isn't captured here.  

There don't appear to be any strong patterns, although there are a few cells with relatively few orders.  

```{r}
order_unique <- bold_meta_filt %>% 
  group_by(cell) %>% 
  filter(!duplicated(order_name)) %>% 
  ungroup()

order_pres <- order_unique %>% 
  mutate(present = 1) %>% # create a dummy column
  pivot_wider(names_from = order_name, 
              values_from = present,
              names_prefix = "pres_") %>% 
  mutate(across(starts_with("pres"), ~if_else(is.na(.), 0, 1))) %>% 
  group_by(cell) %>% 
  summarize(across(starts_with("pres"), ~sum(.x))) 

order_pres_sf <- right_join(gpi, order_pres, by = "cell")

pres_abs_matrix <- order_pres_sf %>% 
  arrange(lat_scaled) %>% 
  as_tibble() %>% 
  select(contains("pres_")) %>% 
  as.matrix() 

dimnames(pres_abs_matrix) <- list(order_pres$cell, colnames(order_pres[,-1]))

# rename_with(~str_replace(., "pres_", "prop_"), starts_with("pres"))

image(t(pres_abs_matrix), axes = FALSE)
```


Now I'm going to visualize proportions! This is the proportion of OTUs that belong to each order within a raster cell.  

```{r}
prop_order <- bold_meta_filt %>% 
  # there are a few observations with no order name
  mutate(order_name = str_replace_all(order_name, "^$", "none")) %>% 
  group_by(cell, order_name) %>% 
  # count the number of unique OTUs per order
  summarize(n_otu = uniqueN(bin_uri)) %>% 
  group_by(cell) %>% 
  # get the total number of OTUs for each cell
  mutate(total_otus = sum(n_otu)) %>% 
  ungroup() %>% 
  mutate(prop_otus = n_otu / total_otus) %>% 
  pivot_wider(names_from = order_name, 
              values_from = prop_otus,
              id_cols = cell,
              names_prefix = "prop_")

prop_order[is.na(prop_order)] <- 0 

prop_order
```

Now, let's plot a matrix similar to the presence-absence matrix. It looks like there is latitudinal bias in sampling, mostly from Diptera and Lepidoptera, with a bit from Hymenoptera. The next course of action is to investigate the genetic diversity of these orders. If there isn't a difference between orders, then we don't have to worry too much about biases in our data.  

```{r}
prop_order_sf <- right_join(gpi, prop_order, by = "cell")

prop_order_matrix <- prop_order_sf %>% 
  arrange(lat_scaled) %>% 
  as_tibble() %>% 
  select(contains("prop_")) %>% 
  as.matrix() 

image(t(prop_order_matrix), 
      col = hcl.colors(12, palette = "viridis"),
      axes = FALSE)

```

#### GD correlations  

GD x total # individuals

Both insignificant
```{r}
# GDE
cor.test(x = bold_cell_sum$hill_1, bold_cell_sum$total_ind)

# GDM
cor.test(x = bold_cell_sum$sqrt_pi, bold_cell_sum$total_ind)
```

GD x median # individuals per OTU

Both insignificant. GDM was initially significant, but the pattern was driven by a single major outlier with over 1200 median # OTUs. (P-value went from 0.026 to 0.4855)
```{r}
# GDE
cor.test(x = bold_cell_sum$hill_1, bold_cell_sum$n_ind_median)

# GDM
cor.test(x = bold_cell_sum$sqrt_pi[bold_cell_sum$n_ind_median < 1200], bold_cell_sum$n_ind_median[bold_cell_sum$n_ind_median < 1200])
```


GD x number of OTUs per cell

Both insignificant
```{r}
# GDE
cor.test(x = bold_cell_sum$hill_1, bold_cell_sum$n_otu)

# GDM
cor.test(x = bold_cell_sum$sqrt_pi, bold_cell_sum$n_otu)
```



### Genetic diversity

Let's take a look at the overall genetic diversity of these orders to see if there is any bias according to the order.  


Read in the pairwise nucleotide difference data  

```{r}
pw_pi <- read_csv(here("output", "spreadsheets", "med_3_150_pi.csv"))

pw_pi
```

Let's make sure we are only including the cells that were used in the analysis.  

Two cells were removed prior to analysis (located in data-poor areas), so we need to remove these from the pairwise pi data set.    

```{r}
analysis_data <- read_csv(here("output", "spreadsheets", "model_data.csv"))

sum(pw_pi$cell %in% analysis_data$cell)
```

Cool.  

```{r}
pw_pi_filt <- pw_pi %>% 
  filter(cell %in% analysis_data$cell)

pw_pi_filt
```


Let's check another bit of our sampling- what is the proportion of each order in the data? The top 6 orders from the data set (out of 26 total orders) contain 97.2 % of the OTUs
```{r}
pw_pi_prop <- pw_pi_filt %>% 
  count(order, sort = TRUE) %>% 
  mutate(prop = n / sum(n),
         order = as.character(order),
         order_summary = ifelse(prop < 0.01, "Other", order),
         order_summary = fct_reorder(order_summary, n)
         )

pw_pi_prop %>% 
  ggplot(aes(x = order_summary, y = prop)) +
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(caption = "\"Other\" includes 20 other insect orders in the data set")
```



```{r}
outlier_orders <- c("Diptera", "Lepidoptera", "Hymenoptera")

pi_ordered <- pw_pi_filt %>% 
  group_by(order) %>% 
  arrange(desc(pi)) %>% 
  mutate(index = row_number()) %>% 
  ungroup()

```


Only outlier orders  

```{r}
pi_ordered %>% 
  filter(order %in% outlier_orders) %>% 
  ggplot(aes(x = index, y = pi, group = order, color = order)) + 
  geom_line() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

All orders  

```{r}
pi_ordered %>% 
  filter(!is.na(order)) %>% 
  ggplot(aes(x = index, y = pi, group = order, color = order)) + 
  geom_line() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

#### GDE

Let's take a look at the Hill numbers. Zygentoma is an outlier because there is only one observation with 4 individuals who all have the same pi.  

```{r}
pw_pi_sum <- pw_pi_filt %>% 
  filter(!is.na(order)) %>% 
  group_by(order) %>% 
  summarize(hill = hill_calc(pi),
            median_pi = median(pi)) %>% 
  mutate(outlier = if_else(
    order %in% outlier_orders, "outlier", "not_outlier"
  ))

pw_pi_sum %>% 
  mutate(order = fct_reorder(order, hill)) %>%
  ggplot(aes(x = order, y = hill, fill = outlier)) +
  geom_col() +
  coord_flip() +
  scale_fill_viridis_d()
```

```{r}
analysis_data %>% 
  summarize(med_hill = median(hill_1))
```

```{r}
pw_pi_filt %>% 
  summarize(hill = hill_calc(pi))

pw_pi_filt %>% 
  filter(order %notin% outlier_orders) %>% 
  summarize(hill = hill_calc(pi))
  
```

Look at Hill number estimates when removing the most common orders. I calculated these using different minimum OTU thresholds because removing the common orders makes certain cells not meet the original filtering criteria.  

```{r}
hill_no_outliers_50 <- pw_pi_filt %>% 
  filter(order %notin% outlier_orders) %>% 
  group_by(cell) %>% 
  filter(uniqueN(bin_uri) >= 50) %>% 
  summarize(hill = hill_calc(pi)) %>% 
  mutate(min_otu = as.character(50))

hill_no_outliers_100 <- pw_pi_filt %>% 
  filter(order %notin% outlier_orders) %>% 
  group_by(cell) %>% 
  filter(uniqueN(bin_uri) >= 100) %>% 
  summarize(hill = hill_calc(pi)) %>% 
  mutate(min_otu = as.character(100))

hill_no_outliers_150 <- pw_pi_filt %>% 
  filter(order %notin% outlier_orders) %>% 
  group_by(cell) %>% 
  filter(uniqueN(bin_uri) >= 150) %>% 
  summarize(hill = hill_calc(pi)) %>% 
  mutate(min_otu = as.character(150))

hill_everything <- pw_pi_filt %>% 
  group_by(cell) %>% 
  summarize(hill = hill_calc(pi)) %>% 
  mutate(min_otu = "everything")

hill_all <- bind_rows(
  hill_no_outliers_50,
  hill_no_outliers_100,
  hill_no_outliers_150,
  hill_everything
) %>% 
  mutate(min_otu = fct_relevel(min_otu, "everything", "50", "100", "150"))

glimpse(hill_all)
```

Boxplot comparing the distributions.  

```{r}
hill_all %>% 
  ggplot(aes(x = min_otu, y = hill)) +
  geom_boxplot(fill = "transparent") +
  geom_jitter(width = 0.25, alpha = 0.8) +
  labs(x = "Minimum number of OTUs",
       y = "GDE") +
  coord_flip() +
  theme_bw()
```



```{r}
hill_no_outliers_150 %>% summarize(median = median(hill),
                                   mean = mean(hill),
                                   iqr = IQR(hill),
                                   sd = sd(hill))

hill_no_outliers_100 %>% summarize(median = median(hill),
                                   mean = mean(hill),
                                   iqr = IQR(hill),
                                   sd = sd(hill))

hill_no_outliers_50 %>% summarize(median = median(hill),
                                  mean = mean(hill),
                                   iqr = IQR(hill),
                                   sd = sd(hill))

hill_everything %>% summarize(median = median(hill),
                              mean = mean(hill),
                                   iqr = IQR(hill),
                                   sd = sd(hill))


```



#### GDM

```{r}
pw_pi_sum %>% 
  mutate(order = fct_reorder(order, median_pi)) %>%
  ggplot(aes(x = order, y = median_pi, fill = outlier)) +
  geom_col() +
  coord_flip() +
  scale_fill_viridis_d()
```





```{r}
gdm_no_outliers_50 <- pw_pi_filt %>% 
  filter(order %notin% outlier_orders) %>% 
  group_by(cell) %>% 
  filter(uniqueN(bin_uri) >= 50) %>% 
  summarize(gdm = median(sqrt(pi))) 

gdm_no_outliers_100 <- pw_pi_filt %>% 
  filter(order %notin% outlier_orders) %>% 
  group_by(cell) %>% 
  filter(uniqueN(bin_uri) >= 100) %>% 
  summarize(gdm = median(sqrt(pi))) 

gdm_no_outliers_100 <- pw_pi_filt %>% 
  filter(order %notin% outlier_orders) %>% 
  group_by(cell) %>% 
  filter(uniqueN(bin_uri) >= 100) %>% 
  summarize(gdm = median(sqrt(pi)))

gdm_no_outliers_150 <- pw_pi_filt %>% 
  filter(order %notin% outlier_orders) %>% 
  group_by(cell) %>% 
  filter(uniqueN(bin_uri) >= 150) %>% 
  summarize(gdm = median(sqrt(pi)))

gdm_everything <- pw_pi_filt %>% 
  group_by(cell) %>% 
  summarize(gdm = median(sqrt(pi)))

ggplot() +
  geom_density(data = gdm_no_outliers_50, aes(x = gdm), color = "black") +
  geom_density(data = gdm_no_outliers_100, aes(x = gdm), color = "purple") +
  geom_density(data = gdm_no_outliers_150, aes(x = gdm), color = "gold") +
  geom_density(data = gdm_everything, aes(x = gdm), color = "green") +
  labs(title = "Green line = all orders\nblack line = outlier orders removed, min_otu = 50\npurple line = outliers removed, min_otu = 100\ngold line = outliers removed, min_otu = 150")
```


```{r}
gdm_no_outliers_150 %>% summarize(median = median(gdm),
                                  mean = mean(gdm),
                                  iqr = IQR(gdm),
                                  sd = sd(gdm))

gdm_no_outliers_100 %>% summarize(median = median(gdm),
                                  mean = mean(gdm),
                                  iqr = IQR(gdm),
                                  sd = sd(gdm))

gdm_no_outliers_50 %>% summarize(median = median(gdm),
                                 mean = mean(gdm),
                                  iqr = IQR(gdm),
                                   sd = sd(gdm))

gdm_everything %>% summarize(median = median(gdm),
                             mean = mean(gdm),
                                  iqr = IQR(gdm),
                                   sd = sd(gdm))
```



### Look at min temp of coldest month

Exploring min temp of the coldest month, which is correlated with max temp of the warmest month- this may be more relevant to the global freezeline than max temp of the warmest month. Move this  section or remove it before publishing  

```{r}
min_temp <- raster(here("data", "climate_agg", "current_medium_bio_6.tif"))

min_temp_obs <- min_temp[analysis_data$cell]


analysis_data$current_medium_bio_6 <- min_temp_obs


summary(lm(current_medium_bio_6 ~ current_medium_bio_5, data = analysis_data))

cor(analysis_data$current_medium_bio_6, analysis_data$current_medium_bio_5)

plot(analysis_data$current_medium_bio_6, analysis_data$current_medium_bio_5)

```


They're correlated, but not as strongly as I'd want to remove one predictor for the other  

```{r}
cor(analysis_data$hill_1, analysis_data$current_medium_bio_5)
cor(analysis_data$hill_1, analysis_data$current_medium_bio_6)

```



## Spatial autocorrelation

Here, I'm exploring spatial autocorrelation in predictors and the response.  

```{r}
new_stab_spatial <- read_csv(here("output", "spreadsheets", "model_data.csv"))
sp_auto_df <- read_sf(here("output", "spreadsheets", "for_figures", "gde_observed.geojson"),
                      crs = "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs") %>% 
  bind_cols(new_stab_spatial)
```


Read in models to look for SAC in residuals  

```{r}
# full reference non-spatial model
model_mult_reg_frz <- read_rds(here("output", "models", "mult_reg_new_stab.rds"))
resid_full <- residuals(model_mult_reg_frz)

# sp auto model
model_top_5_nc <- read_rds(here("output", "models", "glmm_top_5_new_stab.rds"))

pred_sp_auto <- predict(model_top_5_nc)

resid_sp_auto <- model_top_5_nc$y - pred_sp_auto$estimate

# 5 vars without sp auto
mult_reg_frz_5 <- read_rds(here("output", "models", "mult_reg_5.rds"))

resid_5_var <- residuals(mult_reg_frz_5)

# add residuals to spatial df
sp_auto_df <- sp_auto_df %>% 
  mutate(
    full_residuals = resid_full,
    sp_auto_resid_gde = resid_sp_auto,
    resid_5_var
  )
```


```{r}
# create a neighborhood matrix (queen = TRUE means all neighbors, including diagonals, will be included)
sp_nb <- poly2nb(sp_auto_df, queen = TRUE)

# create a weights matrix. style = "W" means that the weights will be scaled from 0-1. This way we can compare across areas with different numbers of areas, which is true for this data set.
# also, ignoring cases with no neighbors. Errors would get thrown otherwise, and since we have islands with no neighbors, this would be a problem
sp_w <- nb2listw(sp_nb, style = "W", zero.policy = TRUE)



plot(sp_nb, coords = st_centroid(sp_auto_df) %>% st_coordinates())
```


Based on these Moran's I plots, it looks like there is spatial autocorrelation in Hill 1 under this strict neighborhood scheme. I plotted it two ways- including zeros and excluding them (turning them to NAs which is why the error appears). Excluding them shows clear spatial patterning;
```{r}
moran.plot(sp_auto_df$hill_1, sp_w,
           xlab = "Hill 1",
           ylab = "Neighbors Hill 1",
           zero.policy = TRUE)

```

First, a quick Moran test to check for spatial autocorrelation. It's better to use the MCMC method since the regression-based method has some strict assumptions that our data likely violates. 

Hill 1 definitely has spatial autocorrelation under this neighborhood scheme.
```{r}

model_vars <- c(
  "current_medium_bio_13",
    "current_medium_bio_14",
    "current_medium_bio_15",
    "current_medium_bio_2",
    "current_medium_bio_5",
    "ghh_medium_std_dev",
    "human_medium_gHM",
    "temp_trend",
    "temp_var", 
    "precip_trend",
    "precip_var",
    "full_residuals",
    "sp_auto_resid_gde",
    "resid_5_var"
)

sp_auto_summary <- sp_auto_df %>% 
  as_tibble() %>% 
  select(all_of(model_vars)) %>%
  map(~moran.mc(.x, sp_w, nsim = 1000, zero.policy = TRUE))

names(sp_auto_summary) <- model_vars

sp_auto_stat <- sp_auto_summary %>% 
  map_dbl(~.x[["statistic"]])

sp_auto_p <- sp_auto_summary %>% 
  map_dbl(~.x[["p.value"]])

sp_auto_tibble <- tibble(
  stat = sp_auto_stat,
  p_value = sp_auto_p,
  predictor = model_vars
) %>% 
  mutate(predictor = fct_reorder(predictor, stat))

sp_auto_tibble %>% 
  ggplot(aes(x = predictor, y = stat)) +
  geom_col() +
  labs(y = "Moran's I") +
  coord_flip()

```

```{r}
sp_corr_gde <- sp.correlogram(sp_nb, sp_auto_df$hill_1, order = 7, method = "I", randomisation = TRUE, zero.policy = TRUE)

print(sp_corr_gde)
plot(sp_corr_gde)
```


```{r}
sp_corr_max_temp <- sp.correlogram(sp_nb, sp_auto_df$current_medium_bio_5, order = 7, method = "I", randomisation = TRUE, zero.policy = TRUE)

print(sp_corr_max_temp)
plot(sp_corr_max_temp)
```

```{r}
sp_corr_max_precip <- sp.correlogram(sp_nb, sp_auto_df$current_medium_bio_13, order = 7, method = "I", randomisation = TRUE, zero.policy = TRUE)

print(sp_corr_max_precip)
plot(sp_corr_max_precip)
```



Moran's I touches zero at around 1,500 km.
```{r}
sp_coords <- st_centroid(sp_auto_df) %>% st_coordinates()
sp_moran_corr <- ncf::spline.correlog(x = sp_coords[,1], 
                                      y = sp_coords[,2], 
                                      z = sp_auto_df$current_medium_bio_15,
                                      resamp = 100,
                                      latlon = FALSE,
                                      xmax = 3e6)

plot(sp_moran_corr, main = "Precip. Seasonality")
```



## Least restrictive data set


Read in and combine data  

```{r}
med_3_df <- read_csv(here("output", "spreadsheets", "cell_medium_3_10_sumstats.csv"))


rast_list_medium <- list.files(here("data", "climate_agg"),
                        pattern = "medium",
                        full.names = TRUE)

new_stab <- list.files(here("data", "climate_agg"), 
                             pattern = "GlobalExtreme", 
                             full.names = TRUE) %>% 
  stack() %>% 
  projectRaster(template_medium_rast)

names(new_stab) <- paste0(names(new_stab), "_medium")

rasters_full_medium <- raster::stack(rast_list_medium, new_stab)
crs(rasters_full_medium) <- "+proj=cea +lon_0=0 +lat_ts=30 +x_0=0 +y_0=0 +datum=WGS84 +ellps=WGS84 +units=m +no_defs"

med_3_filter <- join_predictors(med_3_df, rasters_full_medium, "medium") %>% 
  filter_dfs("medium")

template_medium_obs <- raster(ncols=ncol(rasters_full_medium), nrows=nrow(rasters_full_medium)) %>% 
  projectRaster(template_medium_rast)

template_medium_obs[] <- NA

## replace the values with those in foo
template_medium_obs[med_3_filter$cell] <- rasters_full_medium[[1]][med_3_filter$cell]

template_medium_sf <- template_medium_obs %>% 
  rasterToPolygons() %>% 
  st_as_sf() %>% 
  mutate(cell = sort(med_3_filter$cell))


corr_final_vec <- read_csv(here("output", "spreadsheets", "keep_nocorr_vars.csv")) %>% 
  pull(1) 

corr_final_vec <- corr_final_vec[c(-13, -14)]

med_3_sf <- left_join(template_medium_sf, med_3_filter, by = "cell") %>% 
  rename(
    temp_trend = GlobalExtreme_tsTrendExt_medium,
         temp_var = GlobalExtreme_tsVarExt_medium,
         precip_trend = GlobalExtreme_prTrendExt_medium,
         precip_var = GlobalExtreme_prVarExt_medium
  ) %>% 
  bind_cols(template_medium_sf %>% 
              st_centroid() %>% 
              st_coordinates()) %>% 
  mutate(
    lon_scaled = X * 0.000001,
    lat_scaled = Y * 0.000001
  ) %>% 
  select(
         cell, 
         num_ind,
         num_otu,
         num_order,
         hill_1,
         sqrt_pi,
         any_of(corr_final_vec), 
         temp_trend,
         temp_var,
         precip_trend,
         precip_var,
         X,
         Y,
         lon_scaled,
         lat_scaled)
```



```{r}

all_dfs <- tibble(
  resolution = c(
                 paste(rep("medium", 6, sep = ", "))
                 ),
  min_ind = c(3L, 3L, 3L, 3L, 3L, 3L),
  min_otu = c(10L, 25L, 50L, 100L, 150L, 200L),
  df = list(
    med_3_sf,
    med_3_sf %>% filter(num_otu >= 25),
    med_3_sf %>% filter(num_otu >= 50),
    med_3_sf %>% filter(num_otu >= 100),
    med_3_sf %>% filter(num_otu >= 150),
    med_3_sf %>% filter(num_otu >= 200)
  )
  ) %>% 
  mutate(num_cells = map_int(df, nrow))



```


Normalize all variables so they are centered (mean of 0) and scaled (sd = 1) for linear regression.
```{r}
all_dfs_norm <- all_dfs %>% 
  mutate(df_norm = map(df, normalize_vars)) %>% 
  select(-df)

```


Write to file
```{r, eval=FALSE}
write_rds(all_dfs_norm, here("output", "spreadsheets", "model_data.rds"))
```




